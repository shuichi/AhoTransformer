###############################################################################
# しつこいくらいに行間をぎっしり埋めた丁寧なTransformer Decoder実装例
# decoderonly.py
#
# Shuichi Kurabayashi <shuichi.kurabayashi@keio.jp>
# 
# デコーダのみの Transformer で「Aho 数」判定を学習する実装例。 Aho (エイホ)数と
# は、「3の倍数」または桁に「3」を含む1以上の整数で、落語家の桂三度師匠が発明し
# た、算術演算と文字列照合を組み合わせた極めて高度な数列判定方式です。 これ
# を、encoderonly.pyと同じタスクを、GPT 風のデコーダブロックだけで組みます。な
# お、Transformerデコーダの形状は、原論文に従い、Post-LN（LayerNormが各サブレイ
# ヤーの後にくる形）で実装しています。"Attention Is All You Need"では、各エン
# コーダ／デコーダ層は「マルチヘッド・アテンション・サブレイヤ」と「位置ごとの全
# 結合FFNサブレイヤ」から構成される、と定義されています。
#
# - Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.
#   N., Kaiser, Ł., & Polosukhin, I. (2017). Attention is all you need. In
#   Advances in Neural Information Processing Systems 30 (NeurIPS 2017) (pp.
#   5998–6008).
#
# ※ 以下に登場する (B, S, d_model) はテンソルの形を略記した表記です。
# - B: バッチサイズ（何サンプル分まとめているか）
# - S: シーケンス長（トークン数）
# - d_model: 埋め込みや特徴ベクトルの次元数
###############################################################################



###############################################################################
# 概要
#
# ソースコードを概観すると、Transformerデコーダは、トークナイザ、位置エンコー
# ディング、デコーダーブロック、そして、それらを使って学習を行うトレーニングスク
# リプト(main関数)から構成されています。Transformerデコーダとは、端的には、系列
# （例えば文字列のような、何かの配列）を入力として受け取り、その系列の次に来る
# トークンを予測するモデルです。デコーダは、自己注意機構とフィードフォワードネッ
# トワークを組み合わせたブロックを複数層積み重ねた構造を持ちます。各ブロックは、
# 入力された系列の情報を処理し、文脈に基づいて次のトークンを予測するための内部表
# 現を生成します。デコーダは、「文脈に応じた次のトークンの予測」に優れており、た
# とえば、この例のような数字のAho数の判定、ゲームにおけるNPCの行動の生成、チャッ
# トボットの応答生成、文章の自動生成など、様々なタスクに応用できます。汎用性が高
# いモデルなので、まずはこの例で基本を押さえ、さらに複雑なタスクや大規模データ
# セットに挑戦してみてください。
###############################################################################


###############################################################################
# Transformerデコーダの自己注意(Self-Attention)の計算ステップ解説
#
# ここでは、デコーダー型 Transformerの「自己注意(Self-Attention)」が、因果マスク
# を使ってトークン列 1 2 3 4 5 に対してどう計算されているかを説明します。説明の
# ために、ミニマルな 1 層・1 ヘッドの自己注意をイメージしてください（実際には複
# 数ヘッド・複数層ですが、基本の計算は同じです）。
# 
# まず、トークン列は 5 個です。位置 1〜5 に並んでいるとします。
# - 位置1: 1
# - 位置2: 2
# - 位置3: 3
# - 位置4: 4
# - 位置5: 5
# 
# バッチサイズは 1 として、モデルの隠れ次元を E、自己注意の「キー／クエリの次
# 元」を d_k とします。行列やテンソルの形は次のような記法で書きます。
# - (B, S, E)
# 
# それでは、自己注意の計算ステップを順に見ていきましょう。


# 【（自己注意の前の処理）ステップ1：トークン ID に変換する】
#
# まず文字 1, 2, 3, 4, 5 は、それぞれ語彙（ボキャブラリ）の中の整数 ID に変換さ
# れています。 
# - 1 → 2
# - 2 → 3
# - 3 → 4
# - 4 → 5
# - 5 → 6
# 
# このように、内部的には [2, 3, 4, 5, 6] という整数列ができます。数字を数字のID
# にマッピングしたのでわかりにくいと思いますが、文字列としての数字のトークンのID
# がこの→の右側の数字です。これがシーケンス長 5 の「離散トークン列」です。


# 【（自己注意の前の処理）ステップ2：トークン埋込ベクトルと位置埋込の合成】
# 
# 各トークン ID は、埋め込み行列 E を通して d_model 次元のベクトルに変換されま
# す。E は形が (V, d_model) の行列（V は語彙サイズ）で、行を取り出すと対応する
# トークンのベクトルになります。突然「埋め込み行列 E」が出てきましたが、この埋め
# 込み行列Eは学習されるパラメータです。
# - 1 → e_1
# - 2 → e_2
# - 3 → e_3
# - 4 → e_4
# - 5 → e_5
# 
# それぞれ e_1, e_2, ..., e_5 は長さ d_model のベクトルです。さらにTransformerで
# は、トークンの「位置情報」を与えるために、位置埋め込み（position embedding）も
# 足します。位置1〜5に対応するベクトルを p_1, p_2, p_3, p_4, p_5 とすると、下記
# のように位置情報が埋め込みベクトルに足されます。足されることで、モデルは「この
# ベクトルはシーケンスのどの位置にあるか」を認識できるようになります。
#
# - 位置1: x_1 = e_1 + p_1
# - 位置2: x_2 = e_2 + p_2
# - 位置3: x_3 = e_3 + p_3
# - 位置4: x_4 = e_4 + p_4
# - 位置5: x_5 = e_5 + p_5
# 
# この 5 個のベクトルを縦に並べたものが、自己注意ブロックへの入力 X になります。
# つまり、12345 のトークン列は、行列 X ∈ R^{5×d_model} に変換されま
# す。Transformerにおいてすべての入力は1次元配列ではなく多次元の行列（テンソル）
# になるのです。ここで情報の大幅な拡大が想定されることを意識してください。
# 
# ※ 実装上の注意: Transformerの論文に従い、位置エンコーディングを加算する前に、
# トークン埋め込みベクトル e_i を sqrt(d_model) 倍してスケールを調整しています。
# これにより、学習の初期段階で位置情報が埋め込み情報を打ち消してしまうのを防ぎま
# す。


# 【ステップ3：Q, K, V ベクトルを計算する】
# 
# さて、ここからが自己注意の本番です。自己注意では、同じ入力 X から 3 種類のベク
# トル列を計算します。クエリ Q、キーK、バリュー V です。それぞれは線形変換（全結
# 合層）で求めます。
#
# - Q = X W_Q
# - K = X W_K
# - V = X W_V
#
# ここで W_Q, W_K, W_V は学習される行列で、たとえば形は次のようになります。
#
# - W_Q: (d_model, d_k)
# - W_K: (d_model, d_k)
# - W_V: (d_model, d_v)（普通は d_v = d_k か d_model/h（head数） など）
#
# このように、機械学習において突如未定義のパラメタが出てきたときは、それが「学習
# されるパラメタ」だと理解しておくとスムーズです。機械学習は、基本的に「パラメタ
# を学習する」ことが目的だからです。
#
# 各位置ごとに見ると、次のようになります。各トークンに対応するQKVベクトルが生成
# されていることに注目してください。
#
# - 位置1: q_1 = x_1 W_Q, k_1 = x_1 W_K, v_1 = x_1 W_V
# - 位置2: q_2 = x_2 W_Q, k_2 = x_2 W_K, v_2 = x_2 W_V …
# - 位置5: q_5 = x_5 W_Q, k_5 = x_5 W_K, v_5 = x_5 W_V
# 
# これで、長さ 5 のクエリ列 q_1…q_5、キー列 k_1…k_5、バリュー列 v_1…v_5 ができま
# した。行列としてまとめると、例えば Q は Q ∈ R^{S×d_k} の形を持ちます。


#【ステップ4：全ペアの「類似度スコア行列」を計算する】
# 
# 自己注意の中心になるのが、クエリとキーの内積にもとづいて、トークン間の関係（類
# 似度）を計算する部分です。 「位置 i のクエリ q_i が、位置 j のキー k_j をどれ
# だけ参照するべきか」を、q_i と k_j の内積を使って数値化します。全組み合わせを
# まとめて計算すると、 Scores = Q K^T / sqrt(d_k) という S×S 行列になります
# （バッチ次元を入れると (B, S, S)）。成分で書くと、Scores[i, j] = (q_i · k_j) /
# sqrt(d_k) です。ここで q_i · k_j はベクトルの内積で、「表現空間における向きの
# 近さ」「特徴の類似度」を表します。値が大きいほど、「位置 i から見たときに、位
# 置 j のトークンは文脈的に近い・関連が強い」という意味になります。
# 
# 12345 の例だと、Scores は 5×5 の行列です。マス目 (i, j) には、「位置 i のトー
# クンが、位置 j のトークンに対して持っている相関スコア」が入っています。次に、
# この Scores に対して 次のステップで説明する causal_mask を加算します。


# 【ステップ5：デコーダ特有の「因果マスク（causal mask）」をかける】
# 
# デコーダ型 Transformer では、「未来のトークンを見てはいけない」という制約があ
# ります。位置3 のトークンは、「1」「2」「3」までは見てよいが、「4」「5」は見て
# はいけない、ということです。そのために使うのが「因果マスク」です。長さ 5 の
# シーケンスに対して典型的なマスクは、次のような 5×5 行列です。ここでは False が
# 「見てよい」、True が「未来なので見てはいけない」です。

# False True  True  True  True
# False False True  True  True
# False False False True  True
# False False False False True
# False False False False False
 
# 実装上は、見てはいけない場所（True）に対して非常に大きな負の値（例えば -1e9）
# を足して、softmax をかけたときにほぼ 0 の重みになるようにします。つまり、マス
# ク適用後のスコア行列 Scores_masked は、次のような形になります。
# - 許可された位置 (i, j) では元の Scores[i, j] をそのまま使う
# - 禁止された位置 (i, j) では Scores[i, j] + (-∞ に近い大負数) になり、後続の
#   softmax でほぼ 0 の重みになる
# 
# この時点で、例えば位置3 のクエリ q_3 から見ると、次の要素だけが有限な値で残
# り、Scores_masked[3, 4] と Scores_masked[3, 5] は極端に小さい値になります。
# - Scores_masked[3, 1]（1 に対するスコア）
# - Scores_masked[3, 2]（2 に対するスコア）
# - Scores_masked[3, 3]（3 に対するスコア）
#
# デコーダ型 Transformer で「未来のトークンを見てはいけない」という制約が出てく
# るのは、このモデルの学習のさせ方である、「自己教師あり学習（Self-Supervised
# Learning, SSL）」としての「次トークン予測（Next Token Prediction, NTP）」とい
# うタスクに起因します。デコーダ型 Transformer は、「左から右へ一歩ずつ文章を伸
# ばしていくモデル」です。学習のときには、本物のシーケンス「1 2 3 4 5」が全部わ
# かっているので、それを少しずつズラして、「入力の一部から、次のトークンを当てさ
# せる」という自己教師ありタスクに変換します。位置3について言えば、本来やらせた
# い仕事は「1, 2 という過去のコンテキストから、次に来るべきトークン『3』を予測し
# なさい」です。このとき「正解ラベル」は3ですが、これは別途用意した教師データで
# はなく、入力シーケンス自身から機械的に取り出したものなので、こうした学習設定を
# 自己教師あり学習と呼びます。
# 
# ここで、もし自己注意の計算の中で、位置3が「4 や 5 の埋め込み」も見えてしまった
# らどうなるでしょうか。モデル内部では、「位置3の特徴ベクトル」を作るときに、因
# 果マスクが無ければ「3自身」だけでなく「4, 5」もベクトル内に埋め込めてしまいま
# す。すると、「3 を当てる」というタスクが「過去だけからの予測」ではなく、「すで
# に後ろに書いてある答えを見ながら当てる」タスクになってしまい、情報の漏洩（リー
# ク）が起きてしまいます。そこで、自己注意の部分に「因果マスク（causal mask）」
# を合成し、「位置 t のトークンは、0〜t の位置にだけ注意してよい」「t より後ろの
# 位置は、スコアを強制的にゼロ（実装上はマイナス無限大を足して softmax で消す）
# にする」という制約を入れます。こうすることで、「行列としては 5 個分すべての埋
# め込みを一度に GPU に流して計算する」という実装上の効率は保ちつつも、勾配の流
# れとしては「未来から過去へは情報が伝わらない」ようにできます。つまり、バッチ処
# 理と自己教師あり学習の条件（過去だけを使って次を予測する）を両立させるための仕
# 掛けが、この 5×5 の因果マスク行列だと理解できます。


# 【ステップ6：softmax で「注意重み」を確率分布に変換する】
# 
# 上述の因果マスクを加算したマスク付きスコア行列 Scores_masked を、行ごとに
# softmax にかけます。こうして得られる行列を注意重みと呼びます。
#
# - Attention[i, j] = softmax(Scores_masked[i, :])[j]
# 
# 各行 i は、「位置 i のトークンが、各位置 j のトークンに割り当てる重み」を表す
# 確率分布になっています。値は 0〜1 の間で、行方向に総和 1 になります。例えば位
# 置3 を見ると、
# 
# - Attention[3, 1]：位置3 のトークン（3）から見て、1 をどれだけ重要視するか
# - Attention[3, 2]：同じく 2 をどれだけ重要視するか
# - Attention[3, 3]：3 自身をどれだけ重要視するか
# 
# となり、Attention[3, 4] と Attention[3, 5] はマスクの効果で 0 に近い値になりま
# す。ここで注意したいのは、「単に内積が大きいか小さいか」という生の値を、そのま
# ま使うのではなく、softmax によって「正規化された重み付き係数」として解釈し直し
# ている点です。


# 【ステップ7：注意重みでバリュー列を加重平均して「新しい表現」を作る】
# 
# 次に、この注意重み行列 Attention を使って、バリュー行列 V を加重平均していきま
# す。Output = Attention × V です。成分で書くと、Output[i] = Σ_j Attention[i, j]
# * v_j です。Output[i] は位置 i の最終的な自己注意出力ベクトルで、長さは d_v に
# なります。
# 
# 具体的に位置ごとに見ると、次のように、「各位置の新しい表現ベクトル」は、その位
# 置より前（＋自分自身）のトークンのバリュー v_j を、注意重み Attention[i, j] に
# 従って加重平均したものになります。ここで「コンテキストの識別」が起きています。
# 例えば位置5 のベクトル o_5 は、トークン列 12345 全体の情報を、モデルが重要だと
# 判断したトークンほど強く反映したベクトルになります。3 が文脈的に重要であれば
# Attention[5,3] が大きくなり、その分 v_3 が o_5 に強く相関します。
# - 位置1の出力: o_1 = Attention[1,1] v_1. 未来を見られないので、自分自身 a のバ
#   リューのみの加重平均になります。
# - 位置2の出力: o_2 = Attention[2,1] v_1 + Attention[2,2] v_2. a と b の情報
#   を、学習された比率で混ぜたベクトルです。
# - 位置3の出力: o_3 = Attention[3,1] v_1 + Attention[3,2] v_2 + Attention[3,3]
#   v_3
# - 位置4の出力: o_4 = Attention[4,1] v_1 + Attention[4,2] v_2 + Attention[4,3]
#   v_3 + Attention[4,4] v_4
# - 位置5の出力: o_5 = Attention[5,1] v_1 + Attention[5,2] v_2 + Attention[5,3]
#   v_3 + Attention[5,4] v_4 + Attention[5,5] v_5


# 【ステップ8：FFN（位置ごとの全結合ネットワーク）で非線形変換をかける】
#
# ここまでで自己注意(Self-Attention)は、「各位置のトークンが、過去のどのトークン
# にどれくらい注目するか」を学習し、その重み付き平均によって「文脈を埋め込んだ新
# しい表現ベクトル o_i」を作りました。これはあくまで「線形変換＋加重平均」の組み
# 合わせなので、非線形性が弱く、複雑なパターン（つまりは、非線形な関係性、例えば
# 否定語や数値計算など）を捉えるには不十分です。
#
# そこで、Transformer ブロック内の FFN（Feed-Forward Network、位置ごとの全結合
# ネットワーク）は、この文脈を埋め込んだベクトル o_i を各位置ごとに非線形に変換
# します。具体的には、自己注意ブロックを通過した o_i が、1つ目のLayerNormと残差
# 接続を通過したテンソルX_attn_norm が、形 (B, S, d_model) で FFN に入力されま
# す。FFN の内部は通常、1段目の線形変換として、d_model → dim_feedforward活性化関
# 数、dropout、2段目の線形変換として、dim_feedforward → d_modelという MLP（多層
# パーセプトロン） になっています。1段目の線形変換で特徴次元をいったん大きく（例
# えば d_model=128 に対して dim_feedforward=512 など）広げ、「異なる特徴の組み合
# わせ」を表現できる空間を作成し、活性化関数で非線形な折り曲げを加えたあとに、再
# びd_model 次元に押し戻す、という流れです。このように、特徴次元を一時的に広げる
# ことで、より複雑な表現を獲得した後に、元の次元に戻す処理は、VAE（変分オートエ
# ンコーダ）など他のニューラルネットワークでも頻出するので重要です。
# 
# このとき、1段目の重み行列が「どの特徴を強く組み合わせるか」を学習し、活性化関
# 数が「閾値を超えた特徴だけを強調する」ような役割を担い、2段目の重みが「元の
# d_model の座標系に戻す」役割を担っている、と考えるとイメージしやすいでしょう。
#
# 実装上は、この FFN 出力 Y_ffn に対して再び残差接続と LayerNorm を適用します。
# つまり、 X_ffn_residual = X_attn_norm + Y_ffn X_out =
# LayerNorm(X_ffn_residual) という形です。これにより、「元の情報を壊しすぎずに非
# 線形変換の恩恵だけをうまく取り入れる」「勾配が深い層まで流れやすいようにす
# る」「層をどれだけ積んでも数値スケールが暴れにくくする」といったメリットが得ら
# れます。デコーダブロックを複数層積んでいる場合は、この「自己注意＋FFN（いずれ
# も残差＋LayerNormつき）」の組を何段も繰り返すことで、入力シーケンスの高次な特
# 徴を徐々に獲得していきます。
#
#
# 【ステップ9：最終層（語彙への線形投影と Aho／Safe 判定）】
#
# すべてのデコーダブロックを通過したあとのテンソルを、ここでは H と呼びます。形
# は (B, S, d_model) です。各位置 i にあるベクトル H[:, i, :] は、「その位置まで
# の文脈（Aho 数判定であれば、数字列＋区切りトークンなど）を読み込んだうえで、そ
# の位置のトークンとして何が来るべきかを内部的に保持している」ような表現です。
#
# この H を、最終層の線形変換（いわゆる出力ヘッド、あるいは言語モデルヘッド）に
# 通して、各位置ごとに「語彙の各トークンがどれくらいありそうか」を表すスコア列に
# 変換します。
###############################################################################



###############################################################################
# 【推論時の流れ】
#
# 推論時（実際に Aho 数かどうかを判定したいとき）は、ラベル位置に正解トークンを
# 与えず、「数字列＋[SEP] まで」を入力としてデコーダに通し、その位置に対する
# logits[b, label_pos, :] から最もスコアの高いトークン ID を1つ選びます。これが
# argmax で選んだトークンです。その ID が [AHO] に対応していれば「この数は Aho数
# である」と判断し、[SAFE] に対応していれば「この数は Aho 数ではない」と判断する
# ことができます。内部では、自己注意と FFN を複数層重ねた結果として得られた隠れ
# 表現 H が、「数字列のパターン」と「Aho の定義（3の倍数または3を含む）」を暗黙
# に符号化しており、その情報が最終層の線形変換を通して logits に反映されている、
# という構造になっています。
###############################################################################



###############################################################################
# 予備知識: PyTorch の内部動作概観
#
# PyTorchとは、端的にはnn.Moduleで定義されたニューラルネットワークの計算グラフ
# （Python で書いたモデル）を、 C++（libtorch） の ATen / Dispatcher で各行列演
# 算に分解し、 CUDA カーネルや cuDNN/cuBLAS を呼ぶ処理に置き換える仕組みです。自
# 動微分（Autograd）も同様に、前向き計算中に演算グラフを記録し、 backward 時にそ
# のグラフをたどって勾配計算を行う流れです。
# 
# まずフロントエンドとして、ユーザが（つまりはAIエンジニアや研究者が）nn.Module
# と forward を書いて、model(x) を呼ぶと、学習が行われます。学習中は、forward の
# 中でtorch.matmul や F.relu や self.linear(x) を呼ぶたびに、PyTorch は「個々の
# 演算」をひとつずつ実行していきます。ここでmodel.to("cuda") や x.to("cuda") を
# しておくと、テンソルの device が CUDA になり、以降そのテンソルに対する演算は
# CUDA バックエンドにディスパッチされます。つまりは、学習そのものは逐次的に処理
# されていますが、実態はC++で、さらに実態はCUDA命令で動いている、という構造で
# す。
# 
# テンソルの内部表現は C++(libtorch) の ATen テンソルで、データ型（float32 か
# bfloat16 かなど）とデバイス（CPU, CUDA, MPS…）、サイズ・ストライドなどのメタ
# データを持ち、実データは GPU メモリ上のストレージを指しています。Python から
# torch.matmul を呼ぶと、実際には「ATen の aten::matmul オペレータ」を呼ぶことに
# なり、このオペレータは Dispatcher と呼ばれる仕組みを通って、「どのバックエン
# ド・どのカーネルを使うか」を決めます。device="cuda" のテンソル同士なら CUDA
# バックエンドが選ばれ、CPU なら CPU バックエンドが選ばれます。
# 
# CUDA バックエンド側には、各オペレータごとに「カーネル実装」が用意されていま
# す。行列積や畳み込みのような主要な深層学習の演算については、多くが cuBLAS や
# cuDNN といった NVIDIA 純正ライブラリの関数呼び出しになっていて、そこでさらに
# GPU 世代や形状、stride を見て最適なアルゴリズムが選択されます。PyTorch 側から
# は「conv (畳み込み) をやって」と頼むと、cuDNN が内部でヒューリスティクスやベン
# チマークに基づいて最適な実装を選んでくれる、という構図です。
# 
# 学習の実行時には、CUDA のカーネル呼び出しは基本的に非同期で CUDA ストリームに
# enqueue されます。Python 側から見ると y = model(x) は同期しているように見えま
# すが、内部的には「やることリストを GPU 側に投げておき、.item() などでホストに
# 結果を引き戻すタイミングで同期する」動きになります。PyTorch はメモリアロケータ
# も自前で持っていて、cudaMalloc / cudaFree を頻繁に呼ばないよう、キャッシュ
# allocator で GPU メモリをプールして再利用し、パフォーマンスを落とさないように
# しています。
# 
# 自動微分については、前向き計算中に「どの演算がどのテンソルからどのテンソルを
# 作ったか」というグラフ（Autograd グラフ）を記録していきます。各オペレータには
# 対応する grad_fn（勾配演算）が紐付いていて、loss.backward() を呼ぶと Autograd
# エンジンがこのグラフを逆向きにたどりながら、対応する勾配カーネル（これも CUDA
# バックエンドなら CUDA カーネル＋cuBLAS/cuDNN）を一つずつ起動します。つまり
# forward も backward も、「演算ごとに Dispatcher がバックエンドを選び、CUDAカー
# ネルや cuDNN を叩いていく」という点では同じ構造です。
# 
# ここまではいわゆるeager modeで、演算を 1 個ずつ逐次実行するモードです。これ
# は、PFNのChainerにより広く普及し、PyTorchにも取り込まれ、現在は主流の方式で
# す。PyTorch 2.x 以降は、ここに「コンパイルによる最適化」が加わりま
# す。torch.compile(model) を使うと、TorchDynamo というコンパイラが、が Pythonテ
# ンソル演算部分をフックして、実行中に eager のオペレータ列を FX という中間表現
# のグラフにキャプチャします。その FX グラフに対して、AOTAutograd が前向き計算と
# 勾配計算をまとめて 1 つのグラフとして最適化し、その後ろでTorchInductor がルー
# プレベルのIR（中間表現）に変換します。Inductor は最終的に CUDA カーネルを自動
# 生成し、多くの場合は Triton というDSL を使って一括で fused カーネルを組み立
# て、NVCC/NVRTCで PTX/SASS にコンパイルします。
###############################################################################


import math  # 位置エンコーディングで使う標準数学ライブラリ
import os  # チェックポイント保存用のファイル操作
import random  # 推論テストで乱数を生成する
import sys  # システム操作用標準ライブラリ。今回はexit()のみを使う
from typing import List, Tuple  # 型ヒント用Python標準ライブラリ
# Pythonにおける型ヒント（Type Hints）は、Python 3.5で導入された機能で、変数や関
# 数の引数・戻り値に期待される型を明示的に記述できる仕組みです。

import argparse  # コマンドライン引数処理
import onnx # ONNXライブラリ
import onnxscript  # ONNX スクリプト用ライブラリ
import torch  # PyTorch 本体
import torch.nn as nn  # NN モジュールのショートカット
from torch.utils.data import Dataset, DataLoader  # データセットとローダ
from torch.utils.tensorboard import SummaryWriter # TensorBoard ログ出力用クラス。


###############################################################################
# デコーダ用トークナイザ (Tokenizer)
#
# Transformerにテキスト列を学習させるとき、そのままの文字列では扱えないので、ま
# ずトークンという離散的な ID の列に変換します。ここで問題になるのが、「語彙
# （トークンの種類）をどう設計するか」です。語彙を「単語」だけで構成すると、次の
# ような問題が出ます。文字レベルだけにすると、語彙数は少なくて済む一方で、1 文が
# 非常に長いトークン列になってしまい、学習・推論が非効率になります。単語レベルに
# すると、頻度の低い単語が山ほど出てきて「知らない単語（OOV）」問題が深刻になる
# うえ、語彙サイズも巨大化します。そこで開発されたトークナイザが、BPE（Byte Pair
# Encoding）です。 BPEの基本は、「頻度の高い隣り合うペアを、1 つの新しい記号とし
# て置き換え続ける」ことです。 「よく出る部分文字列は 1 トークンにまとめて短くし
# つつ、めったに出ない単語でも細かく分割すれば必ず表現できる」というバランスを取
# ろうとします。ここでは、BPEのような汎用のトークナイザは使わず、単純に「各桁を1
# トークン」とするトークナイザを実装します。
#
# トークナイザの理解を深めるための参考文献をいくつか紹介します。圧縮アルゴリズム
# の Byte Pair Encoding (BPE)を、ニューラル機械翻訳のサブワード分割に転用し、巨
# 大語彙と未知語問題をうまく処理できることを示した最初の論文は次のものです。
# - Rico Sennrich, Barry Haddow, Alexandra Birch, Neural Machine Translation of
#   Rare Words with Subword Units (ACL 2016)
#
# 次に、サブワード分割が多言語環境において有効であることを示した嚆矢的な論文は、
# 次のものです。ICASSPは音声処理分野の国際会議ですが、この論文では音声検索におけ
# る多言語処理においてサブワード分割が有効であることを示しました。
# - Mike Schuster, Kaisuke Nakajima, Japanese and Korean voice search (ICASSP
#   2012)
# 
# 日本語・韓国語のような単語境界が明示されない言語に対して、サブワード単位の語彙
# を統計的に学習するアプローチ(後の WordPiece)を導入しています。形態素ではなくサ
# ブワード分割が有効であることを示した点で重要です。生のテキスト（空白で単語分割
# しない）」から直接サブワード語彙を学習できる仕組みとしては、SentencePieceが最
# もよく使われています。
#
# - Taku Kudo, John Richardson, SentencePiece: A simple and language independent
#   subword tokenizer and detokenizer for Neural Text Processing (EMNLP 2018) 
# - Taku Kudo, Subword Regularization: Improving Neural Network Translation
#   Models with Multiple Subword Candidates (ACL 2018) 
#
# トークナイザを最初に学ぶ意義は、このトークナイザの独自実装がTransformerの応用
# において極めて重要だということです。例えば、TransformerやLLMは、そのままでは直
# 接的には科学データにてきようすることは困難です。そのため、科学データに特化した
# トークナイザーが必要になります。例えば、化学データのフォーマットとして SMILES
# がよく知られていますが、SMILES には「Cl」「Br」「[nH]」のような複数文字で一単
# 位のトークンが数多くあります。これを単純なバイトレベルや文字レベルのトークンに
# 分割してしまうと、モデルにとっては「C と l の並び」と「Cl（塩素原子）」が区別
# しづらくなり、化学構造が消失する恐れがあります。また、数字は「リング閉じ」のイ
# ンデックスとして使われるなど、自然言語とはかなり違う役割を持ちます。これは、プ
# ログラムコードの学習にも近いため、ドメインごとにトークナイザーを分けることに
# よって、モデルの性能が大きく変わることが知られています。このため、化学系の
# Transformerでは、専用のSMILESトークナイザーを用意して、原子、結合、括弧、イン
# デックスなどをきちんと一単位として切り出す設計が採用されています。ゲーム分野で
# もゲームログに特化したトークナイザを実装することには大きな意義があります。
#
# 分野ごとの独自トークナイザの例を示します。まず典型例が化学、特に SMILES /
# SELFIES 系です。Li & Fourches の SMILES Pair Encoding (SPE) は、「SMILES の頻
# 出部分構造」を BPE 風にまとめてトークンにすることで、原子レベルや単純な文字レ
# ベルよりも、化学的に意味のある単位で分割できるようにしたものです。QSAR や生成
# タスクで、従来のトークン化よりも精度とサンプル効率が改善することを示していま
# す。2024年には、SMILES や SELFIES に対して BPE ではなく Atom Pair Encoding
# (APE) を提案し、化学構造の整合性をより保てると報告している研究や、SPE など既存
# 手法と比較しながら「どのトークナイザーが化学言語モデルに向くか」を系統的に評価
# する仕事も出ています。
#
# - Xinhao Li and Denis Fourches, SMILES Pair Encoding: A Data-Driven
#   Substructure Tokenization Algorithm for Deep Learning, Journal of Chemical
#   Information and Modeling 2021 61 (4), 1560-1569, DOI:
#   10.1021/acs.jcim.0c01127
# - Leon, M., Perezhohin, Y., Peres, F. et al. Comparing SMILES and SELFIES
#   tokenization for enhanced chemical language modeling. Sci Rep 14, 25016
#   (2024). https://doi.org/10.1038/s41598-024-76440-8
# 
# バイオ系でも、アミノ酸や DNA 配列を対象にしたトークナイズ研究が増えています。
# 例えば evoBPE という手法は、単に頻度だけでマージを決めるのではなく「アミノ酸置
# 換行列（進化的に類似なアミノ酸）」に基づいて BPE を拡張し、生物学的に意味のあ
# る単位がトークンとして出てくるようにしたものです。 ここまで来ると、もはや「サ
# ブワードアルゴリズムを、生物学の進化モデルに合わせて再設計する」というレベル
# で、かなりドメイン特化のトークナイザー研究になっています。
#
# - Burak Suyunu, Özdeniz Dolu, and Arzucan Özgür. 2025. evoBPE: Evolutionary
#   Protein Sequence Tokenization. https://doi.org/10.48550/arXiv.2503.08838
# 
# プログラミング言語・ソースコードの世界でも「構文や識別子の性質を意識したトーク
# ン化」が盛んに議論されています。CodeBPE は、ソースコード用の言語モデルに対し
# て、キーワード・識別子・演算子などコード固有のトークン構造を考慮したサブトーク
# ナイズの選択肢を比較し、「どの分割がコードタスクに一番効くか」を体系的に調査し
# ています。
#
# - Nadezhda Chirkova and Sergey Troshin. 2023. CodeBPE: Investigating
#   Subtokenization Options for Large Language Model Pretraining on Source Code.
#   https://doi.org/10.48550/arXiv.2308.00683
#
# 以上のように、トークナイザーは単なる前処理ではなく、モデルが「何を最小要素とす
# るか」をデザインする重要な処理です。色々と創意工夫して、ドメインに最適化した
# トークナイザーを作ることが求められます。
# 
# ところで、LLMにおけるBPEなどのトークナイザー辞書のサイズ（語彙数：Vocabulary
# Size）はどの程度でしょうか？ 2025年における最近のモデルでは、10万を超えるサイ
# ズがトレンドになっています。辞書サイズが小さい場合は、モデルのパラメータ数（特
# にEmbedding層）を小さく抑えられ、学習や推論のメモリ効率が良くなります。一方
# で、未知の単語や複雑な単語を表現するために、細切れのトークン（バイト単位など）
# に分解する必要があります。その結果、1つの文を表すのに必要な「トークン数」が増
# え、文脈（コンテキスト）を消費しやすくなります。現在では、多言語化やマルチモー
# ダル対応、様々なアプリケーションに対応するために、辞書を拡大する傾向にありま
# す。しかし、10Bや100Bを超えるような、いわゆるパラメタ数と異なり、辞書サイズは
# 数十万程度です。
##############################################################################

class DecoderTokenizer:
    """
    デコーダ専用トークナイザ。数字列に [SEP] と [MASK] を付け、最後の位置を予測
    させる。 デコーダーの設計では、batch_first=True を前提としているので、出力テ
    ンソルは (B, S) 形状である。そのため、EncoderのTokenizerと異なり、transpose
    やunsqueezeは不要。

    語彙:
        0: [PAD] 1-10: '0'〜'9' 11: [SEP] 12: [MASK] 13: [AHO] 14: [SAFE]

    Attributes:
        max_len (int): 出力シーケンス長。数字+[SEP]+[LABEL]+PAD をここに収める。
        pad_id (int): PAD トークン ID。 sep_id (int): 区切りトークン ID。
        mask_id (int): 予測用マスク ID。 aho_id (int): Aho ラベルの ID (ターゲット）。
        safe_id (int): Safe ラベルの ID (ターゲット)。 digit2id
        (dict[str, int]): 各数字文字を ID に写像する辞書。
    """

    def __init__(self, max_len: int = 8):
        self.max_len = max_len # 数字列 + [SEP] + [MASK] を収める長さ
        self.pad_id = 0 # PAD トークン
        self.sep_id = 11 # 区切りトークン
        self.mask_id = 12  # 予測させる位置のマスクトークン
        self.aho_id = 13 # AHO ラベル
        self.safe_id = 14 # SAFE ラベル
        self.digit2id = {str(d): d + 1 for d in range(10)}  # [PAD]=0 を避け 1 始まり
        # 0〜9 を文字としてキーにし、値を 1〜10 に割り当てる辞書を生成しています。
        # より詳しく説明すると、この行はPythonの辞書内包表記（Dictionary Comprehension）です。
        # 基本構文は、 {キー式: 値式 for 変数 in イテラブル} です。
        # 1. range(10): 0から9までの整数を生成
        # 2. for d in range(10): 各整数を変数dに代入して繰り返し
        # 3. str(d): キー部分 - 整数dを文字列に変換（"0", "1", "2", ...）
        # 4. d + 1: 値部分 - 整数dに1を加算（1, 2, 3, ...）
        # これにより、数字文字をキー、対応するトークンIDを値とする辞書が生成されます。

    @property
    def vocab_size(self) -> int:
        return 15  # 0〜14の15トークン

    def encode(self, n: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        整数 n を 1 サンプル分のトークン列に変換する。

        Args:
            n (int): 変換する整数。

        Returns:
            Tuple[Tensor, Tensor, Tensor]:
                input_ids: 数字列 + [SEP] + [MASK] + PAD (shape: (max_len,))
                attention_mask: 1=有効, 0=PAD (shape: (max_len,))
                label_id: 正解ラベル [AHO]/[SAFE] のトークン ID (shape: ())
        """
        s = str(abs(int(n))) # 整数を文字列に変換（符号は無視）
        digit_ids = [self.digit2id[ch] for ch in s]  # 各桁を ID に変換
        # 与えられた整数 n を文字列に変換し、その各桁を対応するトークンIDに変換
        # しています。より詳しく説明すると、この行はPythonのリスト内包表記（List
        # Comprehension）です。基本構文は、 [式 for 変数 in イテラブル] です。
        # 1. s: 数字の文字列（例: "123"）
        # 2. for ch in s: 文字列sの各文字を変数chに代入して繰り返し
        # 3. self.digit2id[ch]: 各文字chをキーとして辞書digit2idから対応するIDを
        #    取得
        # 4. [...]: 結果をリストとして生成
        #
        # 具体例もし s = "123"の場合： ch = "1" → self.digit2id["1"] → 2 ch =
        # "2" → self.digit2id["2"] → 3 ch = "3" → self.digit2id["3"] → 4となり、
        # 結果の digit_ids は [2, 3, 4] となります。        
        # 辞書内包表記との違いは次の通りです。辞書内包表記: {キー: 値 for ...} →
        # 辞書を生成リスト内包表記: [式 for ...] → リストを生成

        # 正解ラベル ID を決定（入力には入れずターゲットとして保持）
        if is_aho_number(n):
            label_id = self.aho_id
        else:
            label_id = self.safe_id

        # 入力系列: digits + [SEP] + [MASK]
        tokens = digit_ids + [self.sep_id] + [self.mask_id]
        # ここは単純に、数字トークン列の後ろに区切りトークンとマスクトークンを追
        # 加しています。リスト同士の連結は、+ 演算子で行えます。

        # 長すぎる場合は末尾だけ残す（今回は max_len を十分大きく取る前提）
        if len(tokens) > self.max_len:
            tokens = tokens[-self.max_len:]  # 後ろだけ残す

        attention_mask = [1] * len(tokens) # 有効トークンは 1にする
        # attention_mask は Transformer の自己注意の計算対象に含める位置を示すマ
        # スクとして使われます。1 の位置だけをソフトマックスに入れて、0 の位置
        # （ここでは PAD）を無視することで、余分なパディングが注意スコアや出力に
        # 影響しないようにします。また、多くの実装では損失計算でも同じマスクを
        # 使って PAD を除外します。変数名が attention_mask なの
        # は、PyTorch/Transformers などで慣例的にこの名前が使われており、「どの
        # トークンに attention（自己注意）を向けられるかを示すマスク」という意味
        # づけから来ています。

        # PAD で右側パディング
        while len(tokens) < self.max_len:
            tokens.append(self.pad_id)
            attention_mask.append(0)
            # PAD トークンは無効なので attention_mask は 0 にする

        # Tensor に変換して返す
        input_ids = torch.tensor(tokens, dtype=torch.long)
        attention_mask = torch.tensor(attention_mask, dtype=torch.long)
        label_id_tensor = torch.tensor(label_id, dtype=torch.long)
        return input_ids, attention_mask, label_id_tensor

    def batch_encode(
        self, numbers: List[int]
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        複数の整数をまとめてトークン化し、バッチテンソルを返す。

        Args:
            numbers (List[int]): 変換する整数のリスト。

        Returns:
            Tuple[Tensor, Tensor, Tensor]: 各サンプルの input_ids, attention_mask, label_id を
            先頭軸でまとめたテンソル。
        """
        encoded = [self.encode(n) for n in numbers] # 各整数を個別にエンコード。リスト内包表記でまとめる。
        input_ids = torch.stack([e[0] for e in encoded], dim=0) # 各サンプルの input_ids をバッチ軸で結合
        attention_mask = torch.stack([e[1] for e in encoded], dim=0) # 各サンプルの attention_mask をバッチ軸で結合
        label_ids = torch.stack([e[2] for e in encoded], dim=0) # 各サンプルの label_id をバッチ軸で結合
        return input_ids, attention_mask, label_ids


    def encode_without_label(self, n: int) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        整数 n を 1 サンプル分のトークン列に変換する。

        Args:
            n (int): 変換する整数。

        Returns:
            Tuple[Tensor, Tensor]:
                input_ids: 数字列 + [SEP] + [MASK] + PAD (shape: (max_len,))
                attention_mask: 1=有効, 0=PAD (shape: (max_len,))
        """
        s = str(abs(int(n))) # 整数を文字列に変換（符号は無視）
        digit_ids = [self.digit2id[ch] for ch in s]  # 各桁を ID に変換
        # 与えられた整数 n を文字列に変換し、その各桁を対応するトークンIDに変換
        # しています。より詳しく説明すると、この行はPythonのリスト内包表記（List
        # Comprehension）です。基本構文は、 [式 for 変数 in イテラブル] です。
        # 1. s: 数字の文字列（例: "123"）
        # 2. for ch in s: 文字列sの各文字を変数chに代入して繰り返し
        # 3. self.digit2id[ch]: 各文字chをキーとして辞書digit2idから対応するIDを
        #    取得
        # 4. [...]: 結果をリストとして生成
        #
        # 具体例もし s = "123"の場合： ch = "1" → self.digit2id["1"] → 2 ch =
        # "2" → self.digit2id["2"] → 3 ch = "3" → self.digit2id["3"] → 4となり、
        # 結果の digit_ids は [2, 3, 4] となります。        
        # 辞書内包表記との違いは次の通りです。辞書内包表記: {キー: 値 for ...} →
        # 辞書を生成リスト内包表記: [式 for ...] → リストを生成

        # 入力系列: digits + [SEP] + [MASK]
        tokens = digit_ids + [self.sep_id] + [self.mask_id]
        # ここは単純に、数字トークン列の後ろに区切りトークンとマスクトークンを追
        # 加しています。リスト同士の連結は、+ 演算子で行えます。

        # 長すぎる場合は末尾だけ残す（今回は max_len を十分大きく取る前提）
        if len(tokens) > self.max_len:
            tokens = tokens[-self.max_len:]  # 後ろだけ残す

        attention_mask = [1] * len(tokens) # 有効トークンは 1にする
        # attention_mask は Transformer の自己注意の計算対象に含める位置を示すマ
        # スクとして使われます。1 の位置だけをソフトマックスに入れて、0 の位置
        # （ここでは PAD）を無視することで、余分なパディングが注意スコアや出力に
        # 影響しないようにします。また、多くの実装では損失計算でも同じマスクを
        # 使って PAD を除外します。変数名が attention_mask なの
        # は、PyTorch/Transformers などで慣例的にこの名前が使われており、「どの
        # トークンに attention（自己注意）を向けられるかを示すマスク」という意味
        # づけから来ています。

        # PAD で右側パディング
        while len(tokens) < self.max_len:
            tokens.append(self.pad_id)
            attention_mask.append(0)
            # PAD トークンは無効なので attention_mask は 0 にする

        # Tensor に変換して返す
        input_ids = torch.tensor(tokens, dtype=torch.long)
        attention_mask = torch.tensor(attention_mask, dtype=torch.long)
        return input_ids, attention_mask


    def batch_encode_without_label(
        self, numbers: List[int]
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        複数の整数をまとめてトークン化し、バッチテンソルを返す。

        Args:
            numbers (List[int]): 変換する整数のリスト。

        Returns:
            Tuple[Tensor, Tensor]: 各サンプルの input_ids, attention_mask を先頭
            軸でまとめたテンソル。
        """
        encoded = [self.encode(n) for n in numbers] # 各整数を個別にエンコード。リスト内包表記でまとめる。
        input_ids = torch.stack([e[0] for e in encoded], dim=0) # 各サンプルの input_ids をバッチ軸で結合
        attention_mask = torch.stack([e[1] for e in encoded], dim=0) # 各サンプルの attention_mask をバッチ軸で結合
        return input_ids, attention_mask



###############################################################################
# 位置エンコーディング (Positional Encoding)
#
# 位置エンコーディングは、Decoder-Only Transformer において、シーケンス中の
# 「トークンの並び順」をモデルに教えるための仕組みです。自己注意機構そのものは
# 「順序に不変」であり、トークン埋め込みだけを与えた場合、「どのトークンが何番目
# に現れたか」という情報は失われてしまいます。そこで、各トークンの位置に応じたベ
# クトルをあらかじめ用意し、それを各トークンの埋め込みベクトル(埋め込みトークン)
# に加算することで、順序情報を埋め込みトークンに組み込みます。
#
# ここでは、トークン位置ごとに異なる正弦波（sin / cos）にもとづくベクトルを生成
# し、それを埋め込み X (B, S, d_model) に要素ごと（つまりは、トークン毎）に加算
# します。ここでは、batch_first=True の入力テンソル (B, S, d_model) を前提としま
# す。すると、同じトークンでも位置が違えば異なるベクトル表現になり、モデルは「内
# 容」と「位置」の両方を手がかりに文脈を処理し、次トークンの予測精度を高めること
# ができます。ベクトルへの加算のイメージとしては、「犬」というトークンが高次元空
# 間に位置しているとき、そのベクトル（潜在空間上の点）の周辺も「犬」に対応する領
# 域になりますが、位置エンコーディングでは、その犬の意味を変えない程度にほんの少
# しだけ位置を動かすることで、「トークン先頭の方の"犬"」という情報を与える、とい
# う感じです。
#
# 正弦波にもとづく位置エンコーディングを使う利点は、単に「位置ごとに違う値にな
# る」だけでなく、「位置の差」に対応するパターンが線形な形で埋め込まれている点に
# あります。自己注意ではクエリとキーの内積をとるので、「ある位置 i と別の位置 j
# の距離」も、この位置エンコーディングを通して暗黙的に参照できます。これにより、
# モデルは絶対的な位置だけでなく、「どれくらい離れているか」といった相対的な位置
# 関係も扱えるようになります。
#
# 実装上は、あらかじめ最大系列長 max_len に対して (max_len, d_model) の位置エン
# コーディング行列を作成しておき、順伝播時に入力の系列長 S に合わせて先頭 S 行を
# 取り出し、バッチ方向にブロードキャストして X (B, S, d_model) に加算します。 今
# 回のbatch_first = True の場合では、この加算は「テンソルの形を合わせて足すだ
# け」なので実装は単純です。
###############################################################################

class PositionalEncoding(nn.Module):
    """
    batch_first=True (B, S, E) 前提の位置エンコーディング。

    Attributes:
        pe (Tensor): 事前計算済みの正弦波位置ベクトル（形: (1, max_len, d_model)）。
    """

    def __init__(self, d_model: int, max_len: int = 512):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        # 位置エンコーディングの土台となる配列を用意しています。「最大系列長
        # max_len 行 × 埋め込み次元 d_model 列」の2次元テンソルを0で初期化し、後
        # 続の sin/cos 計算で各位置ごとの値を書き込むための空きテーブルを作って
        # います（デフォルトdtype は float32）。

        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)
        # 0 から max_len-1 までの連番を float32 で作り（形は
        # (max_len,)）、unsqueeze(1) で列方向に次元を1つ足して (max_len, 1) の縦
        # ベクトルにしています。後続で周波数ごとの div_term と掛け算できるよう、
        # 位置 index を列ベクトルに整形している箇所です。

        div_term = torch.exp(
            torch.arange(0, d_model, 2, dtype=torch.float32)
            * (-math.log(10000.0) / d_model)
        )
        # div_term は位置エンコーディングの周波数スケールを並べたベクトルです。
        # torch.arange(0, d_model, 2, dtype=torch.float32) で 0,2,4,… の偶数イン
        # デックスを取り出し（長さは d_model/2）、それに (-math.log(10000.0) /
        # d_model) を掛けて指数のスケールを作り、 torch.exp(...) で exp(-k *
        # log(10000)/d_model) 形式の値列に変換しています。結果は (d_model/2,) 形
        # 状の float32 テンソルで、後続の position * div_term と組み合わせ
        # て、sin/cos の波長を次元ごとに変えた位置エンコーディングを作るために使
        # われます。

        pe[:, 0::2] = torch.sin(position * div_term)
        # 位置エンコーディング行列 pe の偶数次元（0,2,4, … 列）に sin 波を入れて
        # います。position が (max_len, 1)、div_term が (d_model/2,) なのでブ
        # ロードキャストで (max_len, d_model/2) の値が計算され、その結果が偶数カ
        # ラムに丸ごと代入されます（奇数カラムは後続の cos で埋まる）。

        pe[:, 1::2] = torch.cos(position * div_term)
        # 位置エンコーディング行列 pe の奇数次元（1,3,5, … 列）に cos 波を入れて
        # います。position が (max_len, 1)、div_term が (d_model/2,) なのでブ
        # ロードキャストで (max_len, d_model/2) の値が計算され、その結果が奇数カ
        # ラムに丸ごと代入されます（偶数カラムは前の sin で埋まっています）。

        pe = pe.unsqueeze(0) 
        # 先頭にバッチ軸を追加し、形を (1, max_len, d_model) にしています。これ
        # で順伝播時に任意のバッチサイズにブロードキャストしやすくなり、x とその
        # まま足せる形に整えています。

        self.register_buffer("pe", pe)
        # pe を「学習対象ではないがモジュールに属する定数」として登録していま
        # す。これにより、state_dict に含まれて保存・ロードできる（再現性確
        # 保）、to(device) や cuda() でモデルを移したとき一緒にデバイスを移動す
        # る、勾配計算・最適化の対象にならず、学習パラメータと区別できる、などの
        # 利点が得られます。位置エンコーディングは固定値なのでパラメータ登録する
        # 必要はなく、バッファとして持たせるのが適切です。

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        位置エンコーディングを入力に加算する。

        Args:
            x (Tensor): 形状 (B, S, E) の埋め込みベクトル。

        Returns:
            Tensor: 同形状で位置情報を足したテンソル。
        """
        seq_len = x.size(1)
        # x は埋め込みテンソル (B, S, d_model) を想定しているので、x.size(1) で
        # シーケンス長 S（トークン数）を取り出しています。後段で位置エンコーディ
        # ングを切り出す長さとして使うための取得です
        
        return x + self.pe[:, :seq_len] # pyright: ignore[reportIndexIssue]
        # 入力埋め込み x (形 (B, S, d_model)) に、先頭 seq_len 分だけ切り出した
        # 位置エンコーディング self.pe[:, :seq_len] を足して返していま
        # す。self.pe は(1, max_len, d_model) なので、バッチ軸でブロードキャスト
        # され、系列長ぶんだけ位置情報が加算されます。# pyright: ignore[...] は
        # 型チェッカーへの黙らせコメントで、静的型解析の警告を避けるためです。


###############################################################################
# デコーダブロック（自己注意機構のみを持つTransformerブロック）
#
# デコーダブロックは、入力されたトークン列を読み込み、その位置ごとの内部表現（埋
# め込みベクトル）を、一段分“賢く・文脈に即して”更新するための処理単位です。自己
# 注意機構が、因果マスクによって「自分より前にあるトークン」からだけ情報を集めて
# 文脈を混ぜ合わせ（QKVにより長さ n の系列に対して n×n の Attention 行列を計
# 算）、続くフィードフォワードネットワークがその結果を位置ごとに非線形変換して特
# 徴を抽出します。デコーダブロックでの残差接続と正規化は、この更新を安定かつ効率
# 良く積み重ねられるようにする役割を担います。
# 
# これらをブロックとして何層も積み重ねることで、モデルは短い局所的パターンから、
# より長距離の依存関係や抽象的な意味構造まで、段階的に表現を深めながら、次のトー
# クン分布を予測するための高次の文脈表現を形成していきます。今回のAhoTransformer
# では、3の倍数と3の付く数字という簡単な文脈のみを識別するために、2層のデコーダ
# ブロックを用いています。もし、もっと複雑な文脈（条件）を学習させたいなら、3
# 層、4層と積み上げてください。なお、最近のDeepSeekは64層のデコーダブロックを
# 持っています。
# 
# デコーダブロックにおける自己注意機構、すなわち、マルチヘッドアテンション層の内
# 部だけに限ると、入力X に対してやっていることは、本質的には「X → W_Q, W_K, W_V
# で線形変換 → Q/K/Vを作る」だけです。しかし、この Self-Attention は、長さ n の
# 系列に対して n×n のAttention 行列を計算します。つまり計算量もメモリも O(n²) で
# 増えます。系列が 4倍になれば16倍重くなるので、長い文書・コードなどを扱うとすぐ
# メモリと計算量の限界に到達します。特に、メインメモリよりもVRAMの方が制約が大き
# いため、Transformerモデルでは自己注意機能のサイズが常に問題でした。これを回避
# するために、長さ n の系列の全数組み合わせ（n×n）を計算せずに、関連の強いトーク
# ンのみとのAttentionを計算するSparse Attention などの改良方法が数多く提案されて
# おり、DeepSeekなども独自のSparse Attentionを採用しています。
#
# 次に、FFNの発展系としてのMoE(Mixture of Experts)について説明します。MoEは、膨
# 大なパラメタを要求するFFNを置き換え、アクティブになる（計算の対象となる）パラ
# メタを限定することで、使用メモリの総量と推論時のパフォーマンスを低減する方法で
# す。これまでに見てきたように、Transformerでは「自己注意層」と「位置ごとの
# FFN（MLP）」が交互に出てきます。MoEでは、通常はこの FFN 部分を「複数のFFN（=
# experts）」の集合に置き換えます。各トークンは、その層の全 experts を通るのでは
# なく、ルータ（gating network）が選んだ少数の expert だけを通過します。これによ
# り、例えば Mixtral 8×7B では、各層に7Bパラメータ級のFFNが8個ある一方で、各トー
# クンは2個の expertしか通らないので、1トークンあたり実際に使うパラメータはおよ
# そ13B分にとどまりつつ、全体としては 47B パラメータ級のモデル容量を持てる、とい
# う構造になっています。実運用されている LLM のほとんどは、「注意層はそのま
# ま」「FFNをMoEで置き換える」という形を採用しています。つまり、Attention →
# MoE-FFN → 残差 + 正規化という構造になります。
###############################################################################

class DecoderBlock(nn.Module):
    """
    GPT ライクな自己注意機構 + FFN ブロック
    
    Args:
        d_model: 隠れ次元数。
        nhead: マルチヘッド数。
        dim_feedforward: FFN の中間次元。
        dropout: ドロップアウト率。
    """
    # d_modelは、トークン表現の隠れ次元数です。全層で扱うベクトルの長さで、これ
    # を大きくすると表現力は増すが計算・メモリも増えます。一般に256, 512, 1024辺
    # りを採用しておくとうまくいきます。LLMではもっと大きくなり、4096や8192と
    # いった値が使われることもあります。ただし d_model を2倍にすると、重み行列の
    # サイズは概ね4倍になり、FFNやアテンションの計算量・GPUメモリ使用量も一気に
    # 増えるので、小さな実験モデルではまず256〜1024程度から始め、必要に応じて段
    # 階的に増やしていくと安全です。
    # 
    # n_headsは、マルチヘッドアテンションのヘッド数です。d_modelを n_heads で
    # 割った値が各ヘッドの埋め込み次元（head_dim）になるため、d_model % n_heads
    # == 0である必要があります。ヘッド数を増やすと、モデルは「異なる視点の注意パ
    # ターン」を並列に学習できますが、各ヘッドの次元が小さくなりすぎると一つひと
    # つの表現力は落ちます。ヘッド数を増やすと学習が早く進む一方で、精度が落ちる
    # 可能性があるという認識で構いません。実験用には d_model=512 なら n_heads=8
    # や 16 あたりから試すとバランスが取りやすいです。
    # 
    # dim_feedforward: 各Transformerブロック内 FFN（Feed Forward Network）の中間
    # 次元です。FFNは、トークンごとのベクトル x_t（サイズ d_model）に対して
    # 1. 線形変換で d_model → dim_feedforward に拡張
    # 2. GELU や ReLU などの非線形変換
    # 3. 線形変換で dim_feedforward → d_model に縮小という流れの処理を行います。
    # このとき「一時的に広げる先」の次元数がdim_feedforward です。つま
    # り、dim_feedforward を大きくすると、 「1トークンの内部で使える仮想ニューロ
    # ンの数」が増え、その分だけトークンごとに表現できる関数の複雑さ（表現力）が
    # 上がります。多くの実装では、dim_feedforward はd_model の 3〜4倍程度に設定
    # されます。例えば d_model=512 ならdim_feedforward=2048 や1536、d_model=1024
    # ならdim_feedforward=4096 などです。この比率は論文や実装によって微妙に異な
    # り、 2倍弱〜4倍強くらいの範囲で調整されることが多いです。ただ
    # し、dim_feedforward を大きくすると、FFN 内の線形層の重み行列W1: (d_model,
    # dim_feedforward) W2: (dim_feedforward, d_model)のサイズが増えるため、パラ
    # メータ数と計算量（FLOPs）の多くがここで決まります。実務レベルのTransformer
    # では、モデル全体のパラメータのうちかなりの割合がこの FFN部分（つまり
    # dim_feedforward に依存する部分）で消費されます。小さめの実験モデルを作ると
    # きは、
    #
    # 1. まず d_model（隠れ次元）と num_layers（層数）を決める
    # 2. それに対して「どれくらい FFN にパラメータを割くか」
    # 
    # を考え、dim_feedforward ≒ 2〜4 × d_model の範囲で調整するという順序で決め
    # るとバランスを取りやすくなります。あまり大きくしすぎると、Attention よりも
    # FFNが支配的になり、学習コストが重くなる一方で効果が頭打ちになることもある
    # ので、 GPUメモリや学習時間と相談しながら少しずつ増減させるのが安全です。ま
    # とめると、dim_feedforward は「各トークンの内部表現をどれだけリッチに非線形
    # 変換できるか」を決めるつまみであり、モデルの表現力と計算コストを同時に左右
    # する重要なハイパーパラメータです。
    #  
    # dropout: ざっくり言うと、「dropout を下げる＝ランダムにニューロンを消す量
    # を減らす」なので、正則化が弱くなってモデルが“素の能力”を出しやすくなりま
    # す。その結果として、まず学習時の挙動としては、dropout を下げるとネットワー
    # クが受けるノイズが減るので、勾配が安定して訓練損失が下がりやすくなります。
    # これまでdropoutが強すぎて「そもそも十分にフィットできていなかった」場合
    # は、trainだけでなく val/test の精度も一緒に上がることがあります。いわゆる
    # アンダーフィット気味だったのを解消する方向です。一方で、dropout は本来「ラ
    # ンダムにユニットを殺して、過学習を抑える」ための正則化なので、それを弱める
    # とモデルが訓練データにより強くフィットできるようになります。これは良い意味
    # では表現力アップですが、データ量が少ない、ノイズが多い、パラメータ数がやた
    # ら多い、という状況だと「訓練精度だけ上がるけど、検証・テスト精度はむしろ悪
    # 化する」という典型的なオーバーフィット方向の変化になりがちです。理屈として
    # は、dropout が高いと「多数決を取るアンサンブル」を擬似的にやっているような
    # 状態になり、個々のユニットに依存しすぎない頑丈な表現になります。これを下げ
    # ると、そのアンサンブル効果が弱くなり、代わりに特定の特徴や経路に依存した鋭
    # い決定境界を作りやすくなります。そのぶん訓練データには強いが、分布のズレや
    # ノイズには弱くなりがちです。実務的には、もともとのdropout が高すぎて loss
    # が全体に高止まりしているなら下げるメリットが大きく、すでによくフィットして
    # いるモデルでさらにdropout を下げると、短期的には精度アップしても、数エポッ
    # ク後や別データでは悪化する、ということがよくあります。他の正則化（weight
    # decay やデータ拡張など）とのバランスも変わるので、dropout を下げたら
    # 「train/val ロスとメトリクスの差がどう変わるか」を一緒に見るのがおすすめで
    # す。

    def __init__(
        self,
        d_model: int,
        nhead: int,
        dim_feedforward: int = 256,
        dropout: float = 0.05,
    ):
        super().__init__()
        self.self_attn = nn.MultiheadAttention(
            embed_dim=d_model, num_heads=nhead, batch_first=True
        )
        #              Self-Attention: 第1段階
        #     （X から Q / K / V を生成し、ヘッドに分割）
        #     入力埋め込み列 X
        #     形: (B, S, d_model)
        #
        #                     X
        #       （Self-Attention では query = key = value = X）
        #                     │
        #                     │  線形変換（全結合）
        #                     │  W_Q, W_K, W_V
        #         ┌───────────┼───────────────┬───────────────┐
        #         │           │               │               │
        #         ▼           ▼               ▼               ▼
        #    Q_all_heads   K_all_heads    V_all_heads
        # 形: (B, S, d_model) (B, S, d_model) (B, S, d_model)
        #         │               │               │
        #         │   ヘッド方向に分割（d_model = h * d_k）＋ reshape / transpose
        #         │
        #         ├────▶ Q_heads: (B, h, S, d_k)
        #         ├────▶ K_heads: (B, h, S, d_k)
        #         └────▶ V_heads: (B, h, S, d_k)        
        #
        #
        #
        #                     Self-Attention: 第2段階
        #       （各ヘッドでScaled Dot積アテンション → ヘッド結合）
        #
        # Q_heads: (B, h, S, d_k)
        # K_heads: (B, h, S, d_k)
        # V_heads: (B, h, S, d_k)
        #                     │
        #                     │  内積 + スケーリング
        #                     ▼
        #     score = Q_heads @ K_heads^T / sqrt(d_k)
        #     形: (B, h, S, S)
        #                     │
        #                     │  softmax（キー方向 S で正規化）
        #                     ▼
        #     attn_weights = softmax(score, dim = -1)
        #     形: (B, h, S, S)
        #                     │
        #                     │  V_heads との重み付き和
        #                     ▼
        #     head_outputs = attn_weights @ V_heads
        #     形: (B, h, S, d_k)
        #                     │
        #                     │  ヘッド方向の結合（concat）
        #                     ▼
        #     concat = reshape(head_outputs, (B, S, h * d_k))
        #     形: (B, S, d_model)
        #                     │
        #                     │  出力用線形変換 W_O, b_O
        #                     ▼
        #     out = concat @ W_O^T + b_O
        #     形: (B, S, d_model)        
                
        self.linear1 = nn.Linear(d_model, dim_feedforward)
        self.linear2 = nn.Linear(dim_feedforward, d_model)
        self.dropout = nn.Dropout(dropout)
        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.activation = nn.ReLU()
        # Self-Attention（DecoderBlock.forward 内の self.self_attn 付近の行）で
        # は、因果マスクによって各トークンが自分より前のトークンだけにアテンショ
        # ンできるようにしつつ、 key_padding_mask によってパディング位置は無視し
        # ます。これにより系列内の位置をまたいで情報が混ざるため、モデルはプレ
        # フィックス全体（digits + [SEP] + [MASK]）を利用して最終的なラベルトー
        # クンを判断できます。 
        # 
        # Feed-Forward Network（linear1 → activation → dropout → linear2）は、ア
        # テンションの出力に対して位置ごとの非線形変換を適用します。一度
        # dim_feedforward 次元まで拡張し、そこから d_model に再投影することで、
        # 非線形性を加えつつ、アテンションでコンテキスト埋め込まれた後の各トーク
        # ンの特徴量を調整します。より端的には、FFNは「Self-Attentionで収集した
        # 情報を、その位置ごとに“意味のある特徴”に変換するための非線形な変換装
        # 置」です。
        # 
        # まず Self-Attention で「どのトークンがどのトークンからどれくらい情報を
        # もらうか」が決まり（つまり、「Aho」判定という観点からの数字と数字の関
        # 係が決まり）、各位置に「文脈が埋め込まれたベクトル」ができます。ただ、
        # この段階では「足し合わせ+線形変換+softmax」程度の構造なので、表現力と
        # しては限定的です。そこで FFNが、各位置のベクトルに対して独立に、しかし
        # 非線形な多層パーセプトロンとして働き、 「このパターンが来たらこういう
        # 特徴を強調する／抑える」といった形で特徴を再構成します。学習という観点
        # では、たとえば「この桁パターンのときは [AHO] にしたい」 といった、入力
        # と出力の間の複雑な対応関係を、 FFN が各位置ごとの非線形写像としてモデ
        # ル化していると言えます。 Self-Attention が「どの情報を集めるか」を決め
        # る"ルーティング"の役割だとすると、 FFN は「集めてきた情報からどういう
        # 決定境界・特徴抽出を学習するか」を担っている、と考えると分かりやすいと
        # 思います。また、実際のTransformer ではパラメータの大半が FFN に集中し
        # ており、ここがモデルの表現力を大きく左右しています。これまでの実験で
        # も、Self-Attentionを残してFFNを削減すると性能が大きく低下することが知
        # られています。つまり、コンテキストを識別・合成するのがSelf-Attention
        # で、コンテキスト依存のベクトルをタスクに役立つ形に整形・調整するのが
        # FFNです。

    def forward(
        self,
        x: torch.Tensor,
        attn_mask: torch.Tensor,
        key_padding_mask: torch.Tensor,
    ) -> torch.Tensor:
        """
        Args:
            x (Tensor): 入力特徴 (B, S, E)。
            attn_mask (Tensor): 未来を隠す causal mask。shape (S, S)。
            key_padding_mask (Tensor): PAD 位置を True にしたマスク。shape (B, S)。

        Returns:
            Tensor: ブロック通過後の特徴 (B, S, E)。
        """
        # Self-Attention (causal + padding mask)
        attn_output, _ = self.self_attn(
            x,
            x,
            x,
            attn_mask=attn_mask,
            key_padding_mask=key_padding_mask,
            need_weights=False,
        )
        # 自己注意の計算です。
        # - self.self_attn(x, x, x, ...) はクエリ・キー・バリューすべてに同じテ
        #   ンソル x（形: B, S, E）を使う自己注意です。
        # - attn_mask は (S, S) の因果マスクで、未来トークンを見ないよう上三角を
        #   マスクします。
        # - key_padding_mask は (B, S) で PAD 位置を True にし、その位置を無視し
        #   ます。
        # - need_weights=False で注意重み行列を返さず、出力だけ受け取ります。結
        #   果の attn_output（文脈を混ぜた特徴）を元の x に残差接続して、後段の
        #   LayerNorm へ渡しています。
        # 
        # 少し長くなりますが、自己注意機構を詳しく説明します。
        # 
        # まず「クエリ・キー・バリューすべてに同じテンソル x を使う」と書きまし
        # たが、自己注意では、各トークンに対して「何を探すか」を表すベクトル（ク
        # エリ）、 「自分はどういう情報を提供できるか」を表すベクトル（キー）、
        # 「実際に相手に渡す中身」を表すベクトル（バリュー）、この三つを、それぞ
        # れ別々の「学習した変換ルール（バイアス）」を経由して計算します。つま
        # り、変換ルールが学習過程で動的に変化するので、たとえ元の入力ベクトル x
        # が同じデータから来ていても、 QKVの全てが違う、という状態になります。
        # 
        # また、「位置埋め込み」の影響も重要です。数字で言うと「同じ数字"1"で
        # も、文頭にあるか文末にあるかで意味合いが変わる（1009円と9001円は同じ数
        # 字で構成されるが、意味が全く違う）」ので、モデルは各位置に違う位置のラ
        # ベルを足してからSelf-Attentionに渡します。したがって、そもそも列の中の
        # 各トークンは、内容も位置も含めたその場所固有のベクトルになっています。
        # つまり、"位置 i の x"と "位置j のx" は異なる値のベクトルになります。こ
        # れにより、数字の桁数をデコーダーが学習できるようになります。同じ x を
        # 起源としても、最初にランダムに与えられた「クエリ用の変換バイア
        # ス」「キー用の変換バイアス」「バリュー用の変換バイアス」が、損失を小さ
        # くする方向にそれぞれ別々に学習され、調整され続けるため、「クエリ側は "
        # 何を探しているか"を強く表現するように」 「キー側は "どういうときに参照
        # されるべきか" が分かるように」 「バリュー側は "参照されたときに渡すべ
        # き情報" をうまく持てるように」というように、自然と非対称な役割が形成さ
        # れます。最初は何も識別できないネットワークだったとしても、誤差逆伝播を
        # 繰り返すうちに、「このタスクでは、こういうときにここのトークンを見に行
        # くと正解になりやすい」というパターンが埋め込まれていきます。そのうえ
        # で、選ばれたトークンたちから「どんな特徴を受け取るか」はバリューが決
        # め、その結果を後続のFeed-Forward Networkがさらに非線形に加工します。
                
        x = x + self.dropout1(attn_output)
        x = self.norm1(x)        
        # ※ 本実装では、オリジナル論文に忠実な「Post-Norm（Add & Norm）」方式を
        # 採用しています。一方、近年のLLM（GPT-2/3, Llamaなど）では、学習を安定
        # させるためにLayerNormを各サブ層の適用前に行う「Pre-Norm」方式が主流で
        # す。    

        ffn_output = self.linear2(self.dropout(self.activation(self.linear1(x))))
        # フィードフォワードネットワーク (FFN) 部分です。フィードフォワードと
        # は、フィードバックの待機後で、値が戻ることなく、そのまま前に真っ直ぐ伝
        # 搬していくネットワークを意味します。ここでは、各トークンの特徴ベクトル
        # に対して、独立に同じ変換を適用します。具体的には、以下の処理を行ってい
        # ます。
        # - self.linear1(x): 入力ベクトル x を中間次元 dim_feedforward に線形変
        #   換し、特徴を拡張します。
        # - self.activation(...): ReLU や GELU などの活性化関数で非線形性を付与
        #   します。
        # - self.dropout(...): 学習時に一部のユニットを落として正則化します。
        # - self.linear2(...): 中間表現を元の隠れ次元 d_model に戻します。
        # 
        # この一連の処理で、トークンごとの表現を非線形に広げて再圧縮することで、
        # 特徴抽出力を高めています。

        x = x + self.dropout2(ffn_output)
        # FFN の出力に対する残差接続です。 ffn_output は前段の線形1→活性化→ド
        # ロップアウト→線形2で得たトークンごとの変換結果です。self.dropout2 で正
        # 則化したうえで、元の入力 x に足し合わせて（スキップ接続）、勾配消失を
        # 防ぎつつ元情報を保持します。この後の LayerNorm で、足し合わせた結果を
        # 正規化して安定化します。

        x = self.norm2(x)
        return x


###############################################################################
# マスク生成関数
###############################################################################

def generate_subsequent_mask(sz: int, device: torch.device) -> torch.Tensor:
    """
    長さ sz の系列用に、未来を見ないようにする causal mask を作る。

    True / -inf が「見えない」位置になるように MultiheadAttention に渡す。
    PyTorch 2.x では bool マスクも float マスクも受け付ける。
    """
    # shape: (sz, sz)
    # 上三角（対角の1つ上から）を True にして「見えない」位置とする
    mask = torch.triu(torch.ones(sz, sz, dtype=torch.bool, device=device), diagonal=1)
    return mask

    # torch.ones(sz, sz, ...): sz x sz の全要素 1 のテンソルを作りま
    # す。dtype=torch.bool なので全要素が Trueです。torch.triu(..., diagonal=1)
    # は、行列の上三角部分を取り出し、diagonal=1なので主対角線の 1 つ上の斜め線
    # から上側を残し、それ以外を False にします。結果: 下半分と主対角線は
    # False、右上の三角部分だけが True の二値行列。例えば sz=5 の場合、結果は以
    # 下のようになります。

    # False True  True  True  True
    # False False True  True  True
    # False False False True  True
    # False False False False True
    # False False False False False

    # この 5×5 のcausal mask は、「系列長 5 の自己注意において、位置 i のトーク
    # ンが、位置j>i（未来）のトークンに attention できないようにするための
    # False/Trueフラグ行列」であり、各行が「その位置のクエリから見て、どのキー位
    # 置を無効化するか」を指定する役割を果たしています。行方向が「クエリ側のトー
    # クン位置i」、列方向が「キー側のトークン位置 j」を表していると考えてくださ
    # い。ここでは、False が「マスクしない（参照してよい）」、True が「マスクす
    # る（参照してはいけない）」という意味になっています。実装では、この行列を
    # 使って、attention のスコア行列に対して「True のところにだけ非常に大きな負
    # の値を足す」などの処理をし、その後の softmax でその位置への重みがほぼ 0 に
    # なるようにします。
    # 
    # 具体的には、行ごとに次のような意味になります。0 行目は[False True  True
    # True  True] なので、位置 0 のトークンが attention を計算するとき、列 0（自
    # 分自身）へのスコアだけが有効で、列 1,2,3,4（未来の位置）へのスコアはマスク
    # されて使えません。1行目は[False False True  True  True] なので、位置1の
    # トークンは、列0,1（過去と自分自身）には注意できますが、列 2,3,4（さらに未
    # 来）には注意できません。同様に、2 行目 [False False False True  True] は位
    # 置 2 のトークンが0,1,2 だけを参照し、3,4 を見ないことを意味し、3行目[False
    # False False False True] は位置3 が0〜3 だけを参照し、4 行目 [False False
    # False False False]は一番最後のトークンが 0〜4 のすべてを参照できることを意
    # 味します。このパターンは、「行 i から見たとき、列j>iの要素が 1（マスク対
    # 象）、列 j≤iの要素が0（参照可能）」という形になっており、「現在位置より右
    # 側＝未来は全部マスク」というルールを表現しています。このようなマスクを
    # attention のスコアに適用することで、「位置 iの出力ベクトルは、系列中の0〜i
    # 番目のトークン埋め込みだけから計算される」という性質が保証されます。した
    # がって、トークンiの出力は、その位置より右側にあるトークンの情報（まだ予測
    # していない将来のトークン）に依存しません。これによって、デコーダ専用
    # Transformer において、「左から右へ順番にトークンを生成する」という因果構造
    # が保たれ、「次のトークンを予測するときに、答えであるトークンを先に覗き見し
    # ない」という学習上の正しさが確保されています。


###############################################################################
# デコーダ専用 Transformer モデル
###############################################################################

class AhoDecoderTransformer(nn.Module):
    """
    デコーダ単体 Transformer。
    入力: 数字列 + [SEP] + [AHO/SAFE] + PAD
    出力: 各位置の語彙分布（特に最後の [AHO/SAFE] 位置を使う）

    Args:
        vocab_size: 語彙サイズ（14 固定だが外から渡せる形にしている）。
        max_len: シーケンス長。位置エンコーディングやマスク生成で使う。
        d_model: 隠れ次元数。
        nhead: マルチヘッド数。
        num_layers: デコーダブロックの段数。
        dim_feedforward: FFN の中間次元。
        dropout: ドロップアウト率。
    """

    def __init__(
        self,
        vocab_size: int,
        max_len: int,
        d_model: int = 64,
        nhead: int = 4,
        num_layers: int = 2,
        dim_feedforward: int = 256,
        dropout: float = 0.05,
    ):
        super().__init__()
        self.vocab_size = vocab_size
        self.max_len = max_len
        self.d_model = d_model

        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=0)
        self.pos_encoding = PositionalEncoding(d_model, max_len=max_len)

        self.layers = nn.ModuleList(
            [
                DecoderBlock(
                    d_model=d_model,
                    nhead=nhead,
                    dim_feedforward=dim_feedforward,
                    dropout=dropout,
                )
                for _ in range(num_layers)
            ]
        )
        # DecoderBlock(...) for _ in range(num_layers) で、指定された段数だけ同
        # じ構成のデコーダ層を生成（自己注意＋FFN＋残差／LayerNorm入り）していま
        # す。nn.ModuleList に包むことで、PyTorch がこれらの層をサブモジュールと
        # して認識し、パラメータが自動でGPUに転送されます。forward ではこのリス
        # トを順番に回し、各層で隠れ状態 x を更新していくことで深さ方向に表現力
        # を積み上げます。
        
        self.lm_head = nn.Linear(d_model, vocab_size)
        # デコーダ各位置の隠れ表現（次元 d_model）を語彙サイズ vocab_size のロ
        # ジットに射影する出力ヘッド（全結合層）です。入力形状: (B, S, d_model)
        # の各トークンの隠れベクトル。出力形状: (B, S, vocab_size) のスコア
        # （softmax 前）。これを使って、各位置で次に出るトークン（ここでは
        # [AHO]/[SAFE] 含む語彙）の確率分布を計算します。

    def forward(
        self,
        input_ids: torch.Tensor,
        attention_mask: torch.Tensor,
    ) -> torch.Tensor:
        """
        Args:
            input_ids (Tensor): 形状 (B, S) のトークン ID。
            attention_mask (Tensor): 形状 (B, S) のマスク。1=有効, 0=PAD。

        Returns:
            Tensor: 形状 (B, S, vocab_size) のロジット。
        """
        device = input_ids.device
        B, S = input_ids.size()

        x = self.embedding(input_ids) * math.sqrt(self.d_model)  # (B, S, E)
        # nn.EmbeddingでトークンID列を埋め込みベクトル (B, S, E) に変換します。
        # その直後に sqrt(d_model) を掛けてスケーリングし、埋め込みの分散を層の
        # 内部表現スケールに揃えます。このスケーリングはTransformer論文で示され
        # た手法です。これにより位置エンコーディングを足したときの相対的なスケー
        # ルが安定し、初期学習が進みやすくなります。
        
        x = self.pos_encoding(x)
        # 位置エンコーディングを加算。埋め込み済みテンソル x (B, S, E) に事前に
        # 計算した位置エンコーディングを加算して「順序情報」を埋め込む処理です。
        # 自己注意は順序に不変なので、位置ごとに異なる sin/cos ベクトルを足し
        # て、同じトークンでも位置が違えば別の表現になるようにしています。

        attn_mask = generate_subsequent_mask(S, device)
        # 系列長 S に合わせた因果マスク（上三角を True にした (S, S) の bool テ
        # ンソル）を生成する行です。これを MultiheadAttention に渡して、各トーク
        # ンが自分より右（未来）の位置を参照できないようにし、左→右の自己回帰的
        # な挙動を保証します。

        key_padding_mask = attention_mask == 0  # (B, S) bool
        # attention_mask（1=有効トークン、0=PAD）を PyTorch のMultiheadAttention
        # が受け取る形式に変換している行です。細かい意味は以下のとおりです。
        # 
        # 形状と型: attention_mask は (B, S) の long/int テンソルで、各位置が 1
        # (有効) または 0 (PAD)。== 0 をとることで、同じ形 (B, S) の bool テンソ
        # ルを得ています。
        # 
        # True/False の意味づけ: PyTorchにおける MultiheadAttention の
        # key_padding_mask は「無視したい位置を True にする」設計になっていま
        # す。そのため、== 0 で PAD 位置だけがTrue になります。逆に 1の位置（有
        # 効トークン）は False になり、計算対象として残ります。
        # 
        # 何をマスクするか: このマスクは「キー／バリュー側」に適用さ
        # れ、key_padding_mask が True の位置は全クエリから見えなくなります。
        # バッチ内で長さが異なる系列を同じ長さに揃えた際、右側に詰めた PAD部分が
        # 注意計算に混ざるのを防ぎます。他のマスクとの併用: 同じ forward内で
        # attn_mask = generate_subsequent_mask(S, device) を作っていますが、そち
        # らは「未来を隠す」因果マスクです。一方この行で作る key_padding_mask は
        # 「パディングを隠す」マスクで、役割が異なります。MultiheadAttention に
        # 両方渡すことで、未来もPADも同時に無視する自己回帰デコーダの挙動が成立
        # します。

        for layer in self.layers:
            x = layer(
                x,
                attn_mask=attn_mask,
                key_padding_mask=key_padding_mask,
            )
        # このループは、スタックしたデコーダーブロックを順番に通して特徴表現をよ
        # り高次の（抽象度の高い）ベクトル表現へと変換します。これは、ニューラル
        # ネットの複数層を通して、ベクトル表現により高次・広域・非線形な情報をベ
        # クトル内に取り込む処理です。デコーダブロックの層を重ねることで、トーク
        # ン埋め込みは単なる局所的な符号から、文脈を反映した抽象度の高いベクトル
        # へ段階的に変換されます。
        # 
        # 具体的には、各デコーダ層のマルチヘッド自己注意機構により、シーケンスX
        # 内のより遠い文脈や複雑な依存関係を組み合わせた表現へ更新し、位置ごとの
        # FFN（非線形変換）で局所的な基底を再構成・拡張し、残差とLayerNormで勾配
        # とスケールを安定化しつつ、前層の情報を保持・補強します。
        # 
        # FFNについて詳しく説明すると、デコーダの各位置で適用するFFNは、2層（あ
        # るいは多層）の全結合＋非線形変換です。これにより、各トークンの埋め込み
        # を線形写像で別の基底に写し、非線形活性化で特徴を選択／組み合わせ、さら
        # に線形写像で元の次元に戻すという手順で再構成します。自己注意が集めてき
        # た文脈情報を、局所的な座標系（基底）で展開・拡張し直すことで、表現に多
        # 様な非線形成分を付加し、より表現力の高い特徴に作り替える処理を担いま
        # す。
        # 
        # self.layers は__init__ で作ったDecoderBlockのリスト（段数 =
        # num_layers）です。ここでは現在の隠れ表現 x を1 層ずつ渡し、各層が自己
        # 注意＋FFN（内部で残差・LayerNormを含む）で更新したものを次の層へ送りま
        # す。attn_mask は因果マスク（右側＝未来を見ない）、key_padding_maskは
        # PAD を無視するマスクで、全層に共通して渡されています。ループを抜けた後
        # の x が全層を通った最終表現になり、その後lm_head で各トークン位置の語
        # 彙ロジットに変換されます。

        logits = self.lm_head(x)  # (B, S, V)
        # 全層を通した隠れ表現 x（形状 (B, S, d_model)）を語彙次元 V に射影する
        # 線形変換です。lm_head は nn.Linear(d_model, vocab_size) なので、各トー
        # クン位置ごとに [CLS] などを意識せず一括で (B, S, vocab_size) の生のロ
        # ジット（Softmax前のスコア）を出力します。このロジットからクロスエント
        # ロピーで損失を計算したり、Softmaxで確率に変換して次トークンをサンプリ
        # ング／予測します。
        
        return logits


###############################################################################
# ルール: 「Aho」かどうかを判定する関数

# ここでは、「Aho 数」の定義に基づいて、与えられた整数が「3の倍数」または桁に
# 「3」を含むかどうかを判定する関数を実装します。Aho 数の判定は、モデルが学習す
# べき主要なタスクであり、この関数はデータセットのラベル付けやモデルの評価に使用
# されます。
###############################################################################

def is_aho_number(n: int) -> bool:
    """
    与えられた整数が「3の倍数」または桁に「3」を含む場合に True を返す。

    Args:
        n (int): 判定したい整数。負数でもよく、内部で絶対値にして桁を調べる。

    Returns:
        bool: 条件を満たすなら True、それ以外は False。

    str(abs(int(n))) は「整数に直して符号を外し、文字列にして各桁を扱う」処理。
    """
    return (n % 3 == 0) or ("3" in str(abs(int(n))))

###############################################################################
# Dataset
#
# PyTorch の Dataset は、「1件ずつデータを取り出すためのインターフェイス」で
# す。PyTorch は学習データが DataLoader から読み込まれる前提で設計されており、自
# 分で定義するときは、torch.utils.data.Dataset を継承し、__len__ と__getitem__の
# 2つだけ実装することが求められます。具体的には、まず__len__(self) で「データが
# 何件あるか」を返します。次に __getitem__(self, idx)で、「idx 番目のサンプル」
# をタプルもしくは辞書で返します。DataLoaderは内部で for idx in
# range(len(dataset)): のようにインデックスを振りながら__getitem__ を呼び、まと
# めて学習バッチを作成する機能があります。したがって、自作のDataLoaderを渡すだけ
# で、シャッフル・バッチ化・マルチプロセス読み出しなどを PyTorch が自動でやって
# くれます。
###############################################################################

class AhoDecoderDataset(Dataset):
    """
    numbers の各整数を「数字列 + [SEP] + [LABEL]」に変換する Dataset。

    入力: トークン ID 列と attention mask
    ラベル: [AHO] または [SAFE] の ID
    """

    def __init__(self, numbers: List[int], tokenizer: DecoderTokenizer):
        self.numbers = numbers
        self.tokenizer = tokenizer

    def __len__(self):
        return len(self.numbers)

    def __getitem__(self, idx: int):
        n = self.numbers[idx]
        input_ids, attention_mask, label_id = self.tokenizer.encode(n)
        return input_ids, attention_mask, label_id, n
    
    # インデックス idx の整数 n を取り出し、tokenizer.encode でトークン列とマス
    # クとラベルを生成して返します。input_ids, attention_mask, label_id =
    # self.tokenizer.encode(n)は、数字列をトークン化し、PADを除外するためのマス
    # クと [AHO]/[SAFE] ラベルIDを生成します。 encode(n)の戻り値は、この3つのテ
    # ンソルのタプルです。return input_ids, attention_mask, label_id, n: データ
    # ローダに渡す1サンプル分の4タプル。モデル入力（input_ids）、PAD無視用マスク
    # （attention_mask）、教師ラベル（label_id）、元の整数 n（確認用やログ用）で
    # す。


###############################################################################
# 学習ループ
#
# PyTorchの学習ループのメインとなる関数です。これは、1 つのトレーニング エポック
# を実行します。バッチをGPUに転送し、最終的な非PAD位置でロジットを選択し、クロス
# エントロピー損失を計算し、バックプロパゲーションを行って、平均損失を返します。
# 
# クロスエントロピー損失（Cross Entropy Loss、交差エントロピー損失）とは、主に分
# 類問題で使われる損失関数です。正解クラスに高い確率を出すようにモデルを学習させ
# るための、確率分布同士のずれを測る損失関数で、「正解ラベルにどれだけ高い確率を
# 割り当てられているか」を評価し、正解に高い確率を出すほど損失が小さくなります。
# 
# 確率分布の観点から見ると、「真の分布（正解ラベル）」と「モデルが出した分布（予
# 測確率）」の "ずれ" を測る量が交差エントロピーです。深層学習では、1サンプルに
# ついて「正解クラス c に対する予測確率 p(c)」だけを見て損失を -log p(c)と定義し
# ます。正解クラスの確率が 1 に近いほど -log p(c) は 0 に近づきます。一方で、正
# 解クラスの確率が 0 に近いほど損失は非常に大きくなります。マルチクラス分類でよ
# く使う「softmax + クロスエントロピー」は、モデルが出したロジットから softmaxで
# クラスごとの確率分布を作り、その中で正解クラスの確率だけを取り出して -log を取
# る、という処理をまとめて一つの損失として実装したものです。PyTorch の
# nn.CrossEntropyLoss も内部で「logits にsoftmax を適用」＋「正解クラスに対する
# 負の対数尤度」を計算しています。
###############################################################################

def train_one_epoch(
    model: nn.Module,
    dataloader: DataLoader,
    optimizer: torch.optim.Optimizer,
    device: torch.device,
):
    model.train()
    # model.train() でモデルを「学習モード」に切り替えています。PyTorchではこの
    # フラグで挙動が変わり、Dropoutが有効になり、BatchNormなどはバッチ統計で更新
    # されます。評価時は model.eval() にしてこれらを固定します。

    total_loss = 0.0 # 初期化
    total_count = 0 # 初期化

    criterion = nn.CrossEntropyLoss()
    # 損失関数として多クラス分類用の交差エントロピー nn.CrossEntropyLoss を用意
    # しています。後続の loss = criterion(logits_label, label_ids) で、最後の
    # トークン位置のロジットと正解ラベルIDから損失を計算します。

    for input_ids, attention_mask, label_ids, _ in dataloader:
        input_ids = input_ids.to(device) # GPU に転送
        attention_mask = attention_mask.to(device) # GPU に転送
        label_ids = label_ids.to(device) # GPU に転送

        optimizer.zero_grad() # 勾配をゼロにリセット
        logits = model(input_ids, attention_mask)  # (B, S, V)
        # ミニバッチの入力IDとマスクをモデルに通して、各バッチ・各位置の語彙ロ
        # ジット (B, S, V) を出しています。Self-AttentionとFFNを重ねたデコーダの
        # forwardで隠れ表現を作り、最後に線形射影で語彙次元に変換したものが
        # logits です。つまりは、ここが学習のメインです。modelを呼ぶことで、入
        # 力から出力までの一連の計算グラフが構築され、後続の loss.backward() で
        # 誤差逆伝播が可能になります。

        label_positions = attention_mask.sum(dim=1) - 1
        # attention_mask は有効トークンで1、PADで0なので、sum(dim=1) が各系列の
        # 実長を返します。その末尾インデックス（0始まり）がラベル位置なので、- 1
        # して「最後の有効トークン」の位置ベクトル label_positions を作っていま
        # す。label_positions は形 (B,) のラベル位置インデックスです。(B,)とは、
        # テンソルの「次元構造」を表す表記で、B は batch size（バッチサイズ）の
        # 略、(B,)」という書き方は、「長さ B の 1 次元テンソル」という意味です。
                
        B, S, V = logits.size()
        # ロジットのテンソル形状を3要素に分解しています。B=バッチサイズ、S=系列
        # 長、V=語彙サイズで、それぞれ後続のインデックス操作に使います。

        idx = label_positions.unsqueeze(1).unsqueeze(2).expand(-1, 1, V)
        # (B,) を unsqueeze(1)→(B,1)、さらに unsqueeze(2)→(B,1,1) にして gather
        # 用の次元を足し、expand(-1, 1, V) で語彙次元 V にブロードキャストして
        # (B,1,V) 形にしています。こうすることで logits.gather(1, idx) で各バッ
        # チの「ラベル位置」のロジット行（(B,V)）を一括で抜き出せます。
                
        logits_label = logits.gather(1, idx).squeeze(1)  # (B, V)
        # gather(1, idx) で各バッチの「ラベル位置」1行だけを抜き出し、squeeze(1)
        # で余分な次元を落として (B, V) のロジットにしています。これが交差エント
        # ロピーに渡す「正解位置の語彙スコア」です。

        loss = criterion(logits_label, label_ids)
        # 抽出したラベル位置のロジット logits_label と正解クラスID label_ids を
        # 交差エントロピーにかけて損失を計算しています。これがミニバッチ1回分の
        # 分類誤差で、バックプロップに使われます。

        loss.backward()
        # 交差エントロピー損失から勾配を計算し、モデル各パラメータに逆伝播させて
        # います。これにより次の optimizer.step() でパラメータ更新が行える状態に
        # なります。これこそが深層学習における「誤差逆伝播法（バックプロパゲー
        # ション）」の核心で、損失に基づいて各パラメータの勾配が計算され、モデル
        # が学習できるようになります。

        optimizer.step()
        # 逆伝播で溜まった勾配を使って、設定済みの最適化手法（AdamW）が全パラ
        # メータを1ステップ更新する処理です。これこそが機械学習における「学習」の中核で、
        # 勾配に基づいてパラメータを微調整し、損失を減らす方向にモデルを改善します。

        batch_size = input_ids.size(0)
        total_loss += loss.item() * batch_size
        total_count += batch_size
        # 上の3行は、ミニバッチのサイズを input_ids.size(0) で取り出し、そのバッ
        # チの損失合計（loss.item() * batch_size）を total_loss に加算、サンプル
        # 数もtotal_count に積み上げています。最終的に total_loss / total_count
        # で全サンプル平均損失を計算するための累積処理です。

    return total_loss / total_count


# @torch.no_grad() は、この関数内の計算で勾配を記録しないデコレータです。評価・
# 推論用にバックプロップが不要なときに付けることで、requires_grad のテンソルに対
# しても autograd の履歴が作られず、メモリ削減と高速化が得られます。evaluate 関数は
# 学習時の検証ループで使うため、勾配計算が不要なので、ここで指定しています。
@torch.no_grad()
def evaluate(
    model: nn.Module,
    dataloader: DataLoader,
    device: torch.device,
):
    """
    検証用ループ。平均損失と精度を返す。
    """
    model.eval()
    # model.eval() はモデルを推論・評価モードに切り替えます。Dropoutが無効にな
    # り、BatchNorm系は推論用の統計を使うようになり、学習時とは挙動を分けるため
    # のフラグ設定です（勾配計算の有無は torch.no_grad() で制御）。関数先頭のデ
    # コレータ @torch.no_grad() は「勾配を記録しない／逆伝播しない」設定（メモリ
    # 節約・高速化）のためのものですが、それと異なり、model.eval() は「学習時と
    # 挙動が異なる層のモード切替」（Dropoutを止める、BatchNormを推論統計にするな
    # ど）を担います。推論・評価では両方必要で、前者はautograd用、後者はレイヤー
    # の挙動用です。
    
    total_loss = 0.0 # 初期化
    total_count = 0 # 初期化
    total_correct = 0 # 初期化

    criterion = nn.CrossEntropyLoss()
    # 検証ループでも損失計算に多クラス交差エントロピーを使うため、criterion =
    # nn.CrossEntropyLoss() で評価用の損失関数を用意しています。

    for input_ids, attention_mask, label_ids, _ in dataloader:
        input_ids = input_ids.to(device) # GPU に転送
        attention_mask = attention_mask.to(device) # GPU に転送
        label_ids = label_ids.to(device) # GPU に転送

        logits = model(input_ids, attention_mask)  # (B, S, V)
        # 検証バッチをモデルに通し、各トークン位置の語彙ロジット (B, S, V) を得
        # ています。学習時と同じforwardで自己注意＋FFNを通した後、線形射影で語彙
        # 次元に展開した出力が logits です。

        label_positions = attention_mask.sum(dim=1) - 1
        # attention_mask の1の数が系列の実長を表すので、各サンプルの最終有効トー
        # クン位置は sum(dim=1) - 1 になります。その位置が [MASK]（ラベル用）な
        # ので、損失や精度をそこだけで計算するためのインデックスを作っています。
        
        B, S, V = logits.size()
        # ロジットの形 (B, S, V) をバッチ・系列長・語彙サイズの3変数に展開してい
        # ます。この後の gather と精度計算で次元を扱いやすくするための一時変数で
        # す。
        
        idx = label_positions.unsqueeze(1).unsqueeze(2).expand(-1, 1, V)
        # 各サンプルのラベル位置インデックス (B,) を、gather でロジットから取り
        # 出せる形 (B,1,V) にブロードキャストしています。unsqueeze で次元を足
        # し、expand(-1,1,V) で語彙次元に広げることで、logits.gather(1, idx) で
        # ラベル位置の1行をまとめて抜けます。

        logits_label = logits.gather(1, idx).squeeze(1)  # (B, V)
        # gather で各系列のラベル位置のロジット1行を抽出し、squeeze(1) で余分な
        # 次元を落として (B, V) に整形しています。これが検証時に損失と精度を計算
        # する対象のスコア行列です。
        
        loss = criterion(logits_label, label_ids)
        # 各サンプルのラベル位置ロジット (B, V) と正解ラベルID (B,) から、交差エ
        # ントロピーで検証損失を計算しています。学習モードではないので、バックプ
        # ロップはしません。損失を集計して性能を測ることが目的です。

        preds = logits_label.argmax(dim=-1)
        # ラベル位置のロジット (B, V) から、語彙次元に沿って最大スコアのクラスID
        # を取っています。つまり各サンプルの予測ラベル（AHOかSAFE）を preds に求
        # める処理です。
        
        correct = (preds == label_ids).sum().item()
        # 予測ラベル preds と正解 label_ids を要素ごとに比較し、一致した数を合計
        # して整数に変換しています。検証精度を出すための「このバッチで当たった件
        # 数」の計算です。

        batch_size = input_ids.size(0)
        total_loss += loss.item() * batch_size
        total_count += batch_size
        total_correct += correct
        # 上の4行はこの検証バッチの統計を累積しています。batch_size を取得。バッ
        # チ損失の合計を total_loss に加算（平均化のために件数分を掛ける）。サン
        # プル数を total_count に加算。正解数 correct を total_correct に加算。

    return total_loss / total_count, total_correct / total_count


###############################################################################
# 推論: 「Aho」かどうかを表示する関数
#
# 推論関数 aho_infer は、与えられた整数リストをモデルに通し、各整数が「Aho 数」
# かどうかを判定してコンソールに表示します。モデルを eval モードに切り替え、トー
# クナイザで数字列をトークン化し、モデルに通してロジットを得て、最後の有効トーク
# ン位置の出力を抽出し、softmax で確率に変換し、argmax で予測クラスを決定しま
# す。具体的には、例えば、"12345" が入力されたときの入口から出口までのテンソル形
# 状は次のように変化していきます。「12345」が「7×64 の特徴列」になり、それを 2段
# のデコーダブロックで特徴抽出と文脈埋め込みを行い、最後に [MASK] 位置（index
# 6）の 15クラス分類問題として AHO/SAFE を決めている、という流れになります。

# (入力例)
# "12345"
#    │  トークナイズ
#    ▼
# input_ids = [ [ 2, 3, 4, 5, 6, 11, 12 ] ]
# attention_mask = [ [ 1, 1, 1, 1, 1, 1, 1 ] ]
#    shape: (1, 7)
#    │
#    │ Embedding (15 → 64)
#    ▼
# x: 埋め込み＋位置エンコーディング
#    shape: (1, 7, 64)
#    │
#    │  DecoderBlock × 2
#    │    ├ Self-Attn: (1,7,64) → (1,7,64)
#    │    │   内部では QKV: (1,7,64)
#    │    │   → (1,4,7,16) → score (1,4,7,7)
#    │    │   → head_outputs (1,4,7,16) → (1,7,64)
#    │    └ FFN: (1,7,64) → (1,7,256) → (1,7,64)
#    ▼
# x (最終層出力)
#    shape: (1, 7, 64)
#    │
#    │ lm_head (64 → 15)
#    ▼
# logits
#    shape: (1, 7, 15)
#    │
#    │ ラベル位置 index = 6 を抽出
#    ▼
# logits_label
#    shape: (1, 15)
#    │
#    │ softmax
#    ▼
# probs
#    shape: (1, 15)
#    │
#    │ argmax
#    ▼
# pred_id
#    shape: (1,)
#    （13なら AHO, 14なら SAFE）
#
###############################################################################

# @torch.no_grad() は、この関数内の計算で勾配を記録しないデコレータです。評価・
# 推論用にバックプロップが不要なときに付けることで、requires_grad のテンソルに対
# しても autograd の履歴が作られず、メモリ削減と高速化が得られます。aho_infer 関
# 数は純粋に推論のみを行うため、勾配計算が不要なので、ここで指定しています。
@torch.no_grad()
def aho_infer(
    model: AhoDecoderTransformer,
    tokenizer: DecoderTokenizer,
    numbers: List[int],
    device: torch.device,
):
    """
    与えられた整数リストを推論し、結果をコンソールに表示する。
    """
    model.eval()
    # 推論用でも Dropout を止めるなど評価モードにする必要があるため、aho_infer
    # でも model.eval() を呼んでいます。学習時の挙動と区別し、安定した推論を行う
    # ための切り替えです。
    
    input_ids, attention_mask = tokenizer.batch_encode_without_label(numbers)
    # 引数の整数リスト numbers を、トークナイザの batch_encode でまとめてテンソ
    # ル化しています。各数値をトークン列・マスク・ラベルIDに変換し、バッチ次元で
    # スタックした (input_ids, attention_mask, label_ids) を返しています。推論で
    # はこれをそのままモデル入力に使っています（ラベルは精度計算や表示
    # 用）。label_idsが、後続の処理で使われていないことに注意してください。
    
    input_ids = input_ids.to(device) # GPU に転送
    attention_mask = attention_mask.to(device) # GPU に転送

    logits = model(input_ids, attention_mask)
    # 推論用バッチの input_ids と attention_mask をモデルに通し、各位置の語彙ロ
    # ジット (B, S, V) を得ています。学習時と同じ forward で自己注意＋FFNを通
    # し、線形射影で語彙次元に展開した出力が logits です。
    
    label_positions = attention_mask.sum(dim=1) - 1
    # attention_mask の1の数が各系列の実長を表すので、sum(dim=1) - 1 で「最後の
    # 有効トークン（[MASK] 位置）」のインデックスを求めています。推論時もその位
    # 置のロジットだけを取り出して予測に使います。本実装では、分類タスクを解かせ
    # るために、系列の末尾に予測専用のトークン [MASK] を配置し、「この [MASK] が
    # 何（AHO or SAFE）になるべきか」を予測させています。これは通常の文章生成
    # （次の文字を予測し続ける）とは少し異なる、クラス分類用の構成です。
    
    B, S, V = logits.size()
    idx = label_positions.unsqueeze(1).unsqueeze(2).expand(-1, 1, V)
    logits_label = logits.gather(1, idx).squeeze(1)  # (B, V)
    # 上の3行は、ロジットの形 (B, S, V) を展開し、label_positions（形 (B,)）を
    # unsqueeze と expand で (B,1,V) に広げて gather できる形に整えていま
    # す。logits.gather(1, idx) で各サンプルのラベル位置1行だけを抜き出
    # し、squeeze(1) で (B, V) にまとめたのが logits_label です。

    probs = logits_label.softmax(dim=-1)
    preds = probs.argmax(dim=-1).cpu().tolist()
    # 上の2行は、ラベル位置のロジットに softmax をかけて語彙ごとの確率分布 probs
    # を得て、argmax で最尤クラスIDを取り、CPUに戻してPythonリスト化しています。
    # 推論結果として [AHO]/[SAFE] の予測ラベルを取り出す処理です。

    total = len(numbers)
    correct = 0
    fail = 0
    for i, n in enumerate(numbers):
        pred_id = preds[i]
        p_aho = probs[i, tokenizer.aho_id].item()
        p_safe = probs[i, tokenizer.safe_id].item()
        code = "Aho" if is_aho_number(n) else "Safe"
        # 正解を判定するためにPython関数で計算したAho数の判定と比較します。
        if pred_id == tokenizer.aho_id:
            tag = "Aho"
        elif pred_id == tokenizer.safe_id:
            tag = "Safe"
        else:
            tag = f"Other({pred_id})"
        if is_aho_number(n) == (pred_id == tokenizer.aho_id):
            correct += 1
        else:
            fail += 1
        print(
            f"{n:5d} -> {tag} "
            f"(p_AHO={p_aho:.3f}, p_SAFE={p_safe:.3f}, Code={code}, {'OK' if code==tag else 'NG'})"
        )
    acc = correct / total if total > 0 else 0.0
    print(f"\n正解率: {acc*100}% ({correct}/{total}, fail:{fail})")
    # 上のforループは、推論結果を1サンプルずつ読み出して表示と正解カウントを行う
    # 処理です。pred_idはモデル予測ラベル、p_aho/p_safe は softmax から取り出し
    # た AHO/SAFE の確率。予測IDに応じて tag を文字列化（AHO/Safe/その他）。ルー
    # ルベース判定is_aho_number(n) と予測が一致すれば correct をインクリメント。
    # 各行で「入力値 → 予測タグ (確率, ルール判定)」を表示。ループ後に全体の正解
    # 率を出力。


###############################################################################
# メイン関数
###############################################################################

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "-c", "--checkpoint",
        type=str,
        default=None,
        help="学習済みチェックポイントへのパス（指定時は推論のみ実行）",
    )
    parser.add_argument(
        "-i", "--interactive",
        action="store_true",
        help="対話モードで数字を入力して判定（チェックポイント指定時のみ有効）",
    )
    parser.add_argument(
        "-x", "--export",
        type=str,
        default=None,
        help="学習済みチェックポイントへのパスを指定するとONNXへエクスポート",
    )

    args = parser.parse_args()

    if torch.backends.mps.is_available():
        device = torch.device("mps") # Mac (M1/M2/M3 など) のGPU(MPS)
    elif torch.cuda.is_available():
        device = torch.device("cuda") # NVIDIA GPU (Linux / Windows 等)
    else:
        device = torch.device("cpu") # どちらも無ければCPU
    print("device:", device)

    # 5桁まで想定（数字桁最大5 + [SEP] + [LABEL] = 7）なので max_len=7 くらいで足りる
    tokenizer = DecoderTokenizer(max_len=7)

    # 学習用・検証用の範囲
    train_numbers = list(range(1, 40001))
    # 学習には1〜40000の数字を使う
    val_numbers = list(range(40001, 50001))
    # 検証（Validation）には40001〜50000を使う。このように、学習と検証で全く異な
    # るデータセットを使うことで、モデルが単に答えを暗記して対処することを防止す
    # る。

    train_dataset = AhoDecoderDataset(train_numbers, tokenizer)
    val_dataset = AhoDecoderDataset(val_numbers, tokenizer)

    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False)

    model = AhoDecoderTransformer(
        vocab_size=tokenizer.vocab_size,
        max_len=tokenizer.max_len,
        d_model=64,
        nhead=4,
        num_layers=2,       # デコーダブロック数（1にすれば「1ブロックだけ」）
        dim_feedforward=256,
        dropout=0.05,
    ).to(device)
    
    
    if args.export is not None:
        print("\nExporting to ONNX...")
        cpu_device = torch.device("cpu")
        checkpoint = torch.load(args.export, map_location=cpu_device)
        model.load_state_dict(checkpoint["model_state_dict"])
        model.eval()
        model.to(cpu_device)
        print(f"Loaded checkpoint from {args.export} "
              f"(epoch={checkpoint.get('epoch', 'N/A')})")

        # ダミー入力（バッチサイズ1, シーケンス長 max_len）
        dummy_input_ids = torch.zeros(1, tokenizer.max_len, dtype=torch.long, device=cpu_device)
        dummy_attention_mask = torch.ones(1, tokenizer.max_len, dtype=torch.long, device=cpu_device)
        onnx_path = "aho_decoder_transformer.onnx"
        torch.onnx.export(
        model,
            (dummy_input_ids, dummy_attention_mask),
            onnx_path,
            input_names=["input_ids", "attention_mask"],
            output_names=["logits"],   # 形状は (B, S, vocab_size)
            dynamic_axes={
                "input_ids": {0: "batch"},
                "attention_mask": {0: "batch"},
                "logits": {0: "batch"},
            },
            opset_version=17,
            external_data=False,
            dynamo=False,
        )
        print("ONNX エクスポート完了:", onnx_path)
        sys.exit(0)

    # チェックポイント指定時はロードして学習をスキップ
    if args.checkpoint is not None:
        checkpoint = torch.load(args.checkpoint, map_location=device)
        model.load_state_dict(checkpoint["model_state_dict"])
        print(f"Loaded checkpoint from {args.checkpoint} "
              f"(epoch={checkpoint.get('epoch', 'N/A')})")

        model.eval()
        if args.interactive:
            print("\n=== 対話モード ===")
            print("数字を入力してください。空行または Ctrl+C / Ctrl+D で終了します。")
            while True:
                try:
                    raw = input("n> ").strip()
                except (KeyboardInterrupt, EOFError):
                    print("\n終了します。")
                    break

                if raw == "":
                    print("終了します。")
                    break

                try:
                    num = int(raw)
                except ValueError:
                    print("整数を入力してください。")
                    continue

                # 1 サンプルだけをバッチ化して推論する
                input_ids, attention_mask = tokenizer.batch_encode_without_label([num])
                input_ids = input_ids.to(device)
                attention_mask = attention_mask.to(device)

                with torch.no_grad():
                    logits = model(input_ids, attention_mask)
                    label_pos = attention_mask.sum(dim=1) - 1  # 最後の有効トークン
                    _, _, vocab = logits.size()
                    idx = label_pos.unsqueeze(1).unsqueeze(2).expand(-1, 1, vocab)
                    logits_label = logits.gather(1, idx).squeeze(1)  # (1, V)
                    probs = logits_label.softmax(dim=-1)

                pred_id = probs.argmax(dim=-1).item()
                p_aho = probs[0, tokenizer.aho_id].item()
                p_safe = probs[0, tokenizer.safe_id].item()
                pred_is_aho = pred_id == tokenizer.aho_id
                rule = is_aho_number(num)
                tag = "Aho" if pred_is_aho else "Safe"
                correct = " / 正解" if pred_is_aho == rule else ""

                print(
                    f"モデル判定: {num} -> {tag} "
                    f"(p_AHO={p_aho:.3f}, p_SAFE={p_safe:.3f}) "
                    f"/ ルール: {rule}{correct}"
                )
        else:
            print("\n=== デコーダ Transformer によるAho判定 ===")
            test_numbers = list(range(0, 100000))
            aho_infer(model, tokenizer, test_numbers, device)

    else:
        optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4)
        # lr（learning rate, 学習率）一ステップの学習でどれだけパラメータを動か
        # すかの基本スケールです。大きすぎると発散しやすく、小さすぎるといつまで
        # も収束しません。AdamWでは 1e-3 前後がよく使われますが、最近の大きいモ
        # デルだと 1e-4 やそれ以下にすることも多いです。今回は1e-4では小さすぎる
        # のでその倍の2e-4にしています。
        
        num_epochs = 100
        ckpt_dir = "checkpoints"
        os.makedirs(ckpt_dir, exist_ok=True)
        writer = SummaryWriter()
        ckpt_dir = "checkpoints"

        for epoch in range(1, num_epochs + 1):
            train_loss = train_one_epoch(model, train_loader, optimizer, device)
            val_loss, val_acc = evaluate(model, val_loader, device)
            print(
                f"Epoch {epoch:02d} | "
                f"train_loss={train_loss:.4f} | "
                f"val_loss={val_loss:.4f} | "
                f"val_acc={val_acc*100:.2f}%"
            )
            writer.add_scalar("Loss/train", train_loss, epoch)
            writer.add_scalar("Loss/val", val_loss, epoch)
            writer.add_scalar("Accuracy/val", val_acc, epoch)

            if epoch % 10 == 0:
                ckpt_path = os.path.join(ckpt_dir, f"checkpoint_epoch_{epoch}.pth")
                checkpoint = {
                    "epoch": epoch,
                    "model_state_dict": model.state_dict(),
                    "optimizer_state_dict": optimizer.state_dict(),
                }
                torch.save(checkpoint, ckpt_path)
                print(f"Saved checkpoint: {ckpt_path}")
